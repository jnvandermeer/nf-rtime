{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Brain Training\n",
    "This is the notebook where everything regarding NF Training will come together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 modules and where to put files\n",
    "- double check all these parameters\n",
    "- change sub and sess according to the current participant and session (!)\n",
    "- double check the destination directory to be sure\n",
    "- don't use the Template Notebook (if you do so...), but copy/paste a notebook into a subject/session specific directory!\n",
    "- do not do this work just before a measurement. Have it prepared (and preferable tested!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import (most of the) modules that we need\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from multiprocessing import Process\n",
    "import numpy as np\n",
    "import pylsl\n",
    "from datetime import datetime\n",
    "from nftools import guis\n",
    "from nftools.mne import no_bad_samples, no_bad_channels, detect_channel_types\n",
    "from nftools.threshold import find_mode, contiguous_regions, detect_bursts, determine_optimal_threshold\n",
    "import mne\n",
    "import dynarray\n",
    "import pickle\n",
    "from scipy import io as spio\n",
    "import matplotlib.pyplot as plt\n",
    "from pynfb.protocols.ssd.topomap_selector_ica import ICADialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the qt event loop, disable warnings (they flood the screen)\n",
    "%matplotlib qt  \n",
    "# %gui qt\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore') \n",
    "mne.set_config('MNE_LOGGING_LEVEL', 'WARNING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Define sub, ion and run numbers\n",
    "- this will also load all the available data - if they've already been recorded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these according to which sub-sess we have:\n",
    "\n",
    "dataset='BrainTraining' # Rockhampton and Newcastle are other options for this\n",
    "\n",
    "sub = 1\n",
    "ses = 1\n",
    "run = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  /home/johan/nf/rawdata/BrainTraining/bids/sub-01/ses-01/eeg  already exists\n"
     ]
    }
   ],
   "source": [
    "# define where we put our stuff - prepare for BIDS Format Style\n",
    "home_dir = os.path.expanduser('~')\n",
    "save_dir = os.path.join(home_dir, 'nf/rawdata/{}/bids'.format(dataset))\n",
    "\n",
    "this_save_dir = os.path.join(save_dir, 'sub-{:02d}'.format(sub), 'ses-{:02d}'.format(ses), 'eeg')\n",
    "\n",
    "# we should also ... MAKE this savedir! If it exists, we don't do anything.\n",
    "if not os.path.exists(this_save_dir):\n",
    "    os.makedirs(this_save_dir)\n",
    "    print(\"Directory \" , this_save_dir ,  \" Created \")\n",
    "else:    \n",
    "    print(\"Directory \" , this_save_dir ,  \" already exists\")   \n",
    "    \n",
    "    \n",
    "# expected files to be read/written:\n",
    "fname_raw_eo_run = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'eo', run))\n",
    "fname_raw_ec_run = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'ec', run))\n",
    "fname_ica_ocular_rejection = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_ica-ocular-rejection_run-{:02d}.pkl'.format(sub, ses, run))\n",
    "fname_csp_alpha_rejection = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_csp-alpha-rejection_run-{:02d}.pkl'.format(sub, ses, run))\n",
    "\n",
    "fname_calibration_envelopes = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_calibration_envelopes_run-{:02d}.pkl'.format(sub, ses, run))\n",
    "fname_calibration_parameters = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_calibration_parameters_run-{:02d}.pkl'.format(sub, ses, run))\n",
    "\n",
    "fname_raw_nftraining_run = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'raw-nftraining', run))\n",
    "fname_rtanalyzed_nftraining_eeg = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'rtanalyzed-nftraining-eeg', run))\n",
    "fname_rtanalyzed_nftraining_emg = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'rtanalyzed-nftraining-emg', run))\n",
    "\n",
    "\n",
    "# if files exist, load them\n",
    "if os.path.exists(fname_raw_eo_run):\n",
    "    print('found raw eyes open: \\t\\t' + os.path.basename(fname_raw_eo_run))\n",
    "    raw_eo = mne.io.read_raw_fif(fname_raw_eo_run, preload=True)\n",
    "if os.path.exists(fname_raw_ec_run):\n",
    "    print('found raw eyes closed: \\t\\t' + os.path.basename(fname_raw_ec_run))\n",
    "    raw_ec = mne.io.read_raw_fif(fname_raw_ec_run, preload=True)\n",
    "if os.path.exists(fname_ica_ocular_rejection):\n",
    "    print('found ica eyeblink rejection:\\t' + os.path.basename(fname_ica_ocular_rejection))\n",
    "    with open(fname_ica_ocular_rejection,'rb') as f:\n",
    "        ica_rejection = pickle.load(f)\n",
    "if os.path.exists(fname_csp_alpha_rejection):\n",
    "    print('found csp alpha rejection: \\t' + os.path.basename(fname_csp_alpha_rejection))\n",
    "    with open(fname_csp_alpha_rejection, 'rb') as f:\n",
    "        csp_rejection = pickle.load(f)\n",
    "\n",
    "if os.path.exists(fname_calibration_envelopes):\n",
    "    print('found calibration envelopes: \\t' + os.path.basename(fname_calibration_envelopes))\n",
    "    with open(fname_calibration_envelopes, 'rb') as f: \n",
    "        calibration_envelopes = pickle.load(f)\n",
    "    locals().update(calibration_envelopes)\n",
    "if os.path.exists(fname_calibration_parameters):\n",
    "    print('found calibration parameters: \\t' + os.path.basename(fname_calibration_parameters))\n",
    "    with open(fname_calibration_parameters, 'rb') as f: \n",
    "        calibration_parameters = pickle.load(f)\n",
    "    locals().update(calibration_parameters)\n",
    "        \n",
    "if os.path.exists(fname_raw_nftraining_run):\n",
    "    print('found raw nftraining: \\t\\t' + os.path.basename(fname_raw_nftraining_run))\n",
    "    raw_nftraining = mne.io.read_raw_fif(fname_raw_nftraining_run)\n",
    "if os.path.exists(fname_rtanalyzed_nftraining_eeg):\n",
    "    print('found rt-analyzed eeg: \\t\\t' + os.path.basename(fname_rtanalyzed_nftraining_eeg))\n",
    "    rtanalyzed_nftraining_eeg = mne.io.read_raw_fif(fname_rtanalyzed_nftraining_eeg)\n",
    "if os.path.exists(fname_rtanalyzed_nftraining_emg):\n",
    "    print('found rt-analyzed emg: \\t\\t' + os.path.basename(fname_rtanalyzed_nftraining_emg))\n",
    "    rtanalyzed_nftraining_emg = mne.io.read_raw_fif(fname_rtanalyzed_nftraining_emg)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Fix/check the EEG Cap\n",
    "\n",
    "- fix the EEG Cap\n",
    "- check with openBCI GUI if the signals look OK, once they do:\n",
    "- run the `python raw_eo_data --stream`, followed by `/start`\n",
    "- the light on the usb stick should go <font color=\"red\">RED</font>\n",
    "- (re)-start the Cap or USB if it doesn't work, followed by commands above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Connect to the real-time Data Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pylsl.pylsl.StreamInfo at 0x7f0642657d30>,\n",
       " <pylsl.pylsl.StreamInfo at 0x7f064c957278>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pylsl.resolve_streams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['openbci_eeg_id76', 'openbci_aux_id76']\n"
     ]
    }
   ],
   "source": [
    "# prints out which streams are currently available\n",
    "stream_ids = [ pylsl.stream_inlet(s).info().source_id() for s in pylsl.resolve_streams() ]\n",
    "print(stream_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- copy/paste the eeg lab name (left of the 2 outputs) into stream_id variable:\n",
    "- thake the one that says **eeg**, not aux!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openbci_eeg_id76\n"
     ]
    }
   ],
   "source": [
    "# 'subscribe' to a data stream; grab all essential(s), fs, names, etc\n",
    "# stream_id = 'openbci_eeg_id134'\n",
    "\n",
    "# eeg_stream_id = [stream_id for stream_id in stream_ids if re.match('.*_eeg_.*', stream_id)]\n",
    "eeg_stream_id = [stream_id for stream_id in stream_ids if re.match('.*eeg.*', stream_id)]\n",
    "if len(eeg_stream_id)>0:\n",
    "    eeg_stream_id=eeg_stream_id[0]\n",
    "print(eeg_stream_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name = openbci_eeg\n",
      "sampling_freq = 125\n",
      "channel_count = 16\n",
      "channel_format = 1\n"
     ]
    }
   ],
   "source": [
    "# try grabbing all the information from that stream:\n",
    "data_stream=pylsl.resolve_byprop(\"source_id\", eeg_stream_id, timeout=5.0)\n",
    "if data_stream:\n",
    "    data_inlet=pylsl.stream_inlet(data_stream[0], max_buflen=10)\n",
    "    stream_info = data_inlet.info()\n",
    "    stream_Fs = stream_info.nominal_srate()\n",
    "    stream_xml = stream_info.desc()\n",
    "    chans_xml = stream_xml.child(\"channels\")\n",
    "    chan_xml_list = []\n",
    "    ch = chans_xml.child(\"channel\")\n",
    "    while ch.name() == \"channel\":\n",
    "        chan_xml_list.append(ch)\n",
    "        ch = ch.next_sibling(\"channel\")\n",
    "    channel_names = [ch_xml.child_value(\"label\") for ch_xml in chan_xml_list]\n",
    "    data_inlet_dt = data_inlet.time_correction(timeout=5.0)\n",
    "    sampling_freq = data_stream[0].nominal_srate()\n",
    "    print('name = %s' % data_stream[0].name())\n",
    "    print('sampling_freq = %d' % sampling_freq)\n",
    "    print('channel_count = %d' % data_stream[0].channel_count())\n",
    "    print('channel_format = %d' % data_stream[0].channel_format())\n",
    "else:\n",
    "    raise Exception('No Data Stream Found - Is your EEG Cap running?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data acquisition for Calibrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Measure Some Data (Eye Open)\n",
    "- Inform that the following measurement is an eye open measurement.\n",
    "- Subjects are allowed to blink as normal\n",
    "- duration about 2 minutes\n",
    "- might have to re-run this in order to fix a channel from being busted\n",
    "- when ready press 'stop acquisition' and close window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put the incoming data in here:\n",
    "np_eo = dynarray.DynamicArray((None, len(channel_names))); times_of_samples = []\n",
    "\n",
    "# init/open the window in which we visualize data\n",
    "w=guis.AcquireData(sampling_freq, channel_names)\n",
    "\n",
    "# before we start, pull everything from the buffer (empty it)\n",
    "data_inlet.pull_chunk()\n",
    "while data_inlet.samples_available(): data_inlet.pull_chunk() \n",
    "    \n",
    "# then start acquiring data as long as button 'stop' not pressed:\n",
    "w.RUNLOOP=True\n",
    "while w.RUNLOOP:\n",
    "\n",
    "    time.sleep(0.001)\n",
    "    if not data_inlet.samples_available():\n",
    "        w.update(None)\n",
    "    else:\n",
    "        chunk_data, chunk_times = data_inlet.pull_chunk(timeout=0.0) # grab from LSL\n",
    "\n",
    "        \n",
    "        np_eo.extend(chunk_data) # add to our list\n",
    "        times_of_samples.append(chunk_times)\n",
    "        w.update(chunk_data) # update the GUI window       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RawArray  |  None, n_channels x n_times : 18 x 7875 (63.0 sec), ~1.1 MB, data loaded>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we make the MNE data file from it\n",
    "raw_eo = mne.io.RawArray(np.transpose(np_eo)*1E-6,\n",
    "                        mne.create_info(channel_names, \n",
    "                                    sampling_freq, \n",
    "                                    detect_channel_types(channel_names), \n",
    "                                    'standard_1020'\n",
    "                        )\n",
    "                   )\n",
    "\n",
    "# we make an lsl packet membership so we can check transmission statistics\n",
    "lsl_packet = np.array([i \n",
    "          for i, c in enumerate([len (x) for x in times_of_samples]) \n",
    "          for n in range(c)]\n",
    "        )\n",
    "\n",
    "# make a lsl timestamp signal (essential later on)\n",
    "lsl_time = np.concatenate(times_of_samples) - times_of_samples[0][0]\n",
    "\n",
    "# We add this infmrmation to the raw.\n",
    "info_lslinfo = mne.create_info(('lsl_time','lsl_packet'), sampling_freq, ('misc','misc'))\n",
    "raw_lslinfo = mne.io.RawArray((lsl_time, lsl_packet), info_lslinfo)\n",
    "raw_eo.add_channels([raw_lslinfo])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Inspect the eo data  (run1)\n",
    "- scroll through the data see if the EEG signal is what you expect\n",
    "- mark bad channels (that cannot be rescued)\n",
    "- mark bad segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a simple display filter\n",
    "raw_eo_copy = raw_eo.copy()\n",
    "raw_eo_copy.filter(1, 40).notch_filter((25, 50)).plot(title='eyes open');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save this raw data to disk (filename taken care of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-01/ses-01/eeg/sub-01_ses-01_task-eo_run-13.fif\n"
     ]
    }
   ],
   "source": [
    "raw_eo.set_annotations(raw_eo_copy.annotations)\n",
    "raw_eo.info['bads'] = raw_eo_copy.info['bads']\n",
    "\n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'eo', run)\n",
    "fname_raw_eo_run = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "raw_eo.save(fname_raw_eo_run, overwrite=True)\n",
    "\n",
    "print('saved: ' + fname_raw_eo_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Measure Some Data (Eye Closed)\n",
    "- Inform your subject to keep the eye closed, then start the measurement\n",
    "- duration about 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put the incoming data in here:\n",
    "np_ec = dynarray.DynamicArray((None, len(channel_names))); times_of_samples = []\n",
    "\n",
    "\n",
    "# init/open the window in which we visualize data\n",
    "w=guis.AcquireData(sampling_freq, channel_names)\n",
    "\n",
    "# before we start, pull everything from the buffer (empty it)\n",
    "data_inlet.pull_chunk()\n",
    "while data_inlet.samples_available(): data_inlet.pull_chunk() \n",
    "    \n",
    "# then start acquiring data as long as button 'stop' not pressed:\n",
    "w.RUNLOOP=True\n",
    "while w.RUNLOOP:\n",
    "\n",
    "    time.sleep(0.001)\n",
    "    if not data_inlet.samples_available():\n",
    "        w.update(None)\n",
    "    else:\n",
    "        chunk_data, chunk_times = data_inlet.pull_chunk(timeout=0.0) # grab from LSL\n",
    "\n",
    "        np_ec.extend(chunk_data) # add to our list\n",
    "        times_of_samples.append(chunk_times)\n",
    "        w.update(chunk_data) # update the GUI window        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting bad chanels in the data set to be the same as previous dataset\n",
      "[]\n",
      "Do not change bad channels further - re-do eo and ec if needed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RawArray  |  None, n_channels x n_times : 18 x 7137 (57.1 sec), ~1.0 MB, data loaded>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we make the MNE data file from it        \n",
    "raw_ec = mne.io.RawArray(np.transpose(np_ec)*1E-6,\n",
    "                        mne.create_info(channel_names, \n",
    "                                    sampling_freq, \n",
    "                                    detect_channel_types(channel_names), \n",
    "                                    'standard_1020'\n",
    "                               )\n",
    "                       )\n",
    "raw_ec.info['bads'] = raw_eo.info['bads']\n",
    "print('setting bad chanels in the data set to be the same as previous dataset')\n",
    "print(raw_ec.info['bads'])\n",
    "print('Do not change bad channels further - re-do eo and ec if needed!')\n",
    "\n",
    "# we make an lsl packet membership so we can check transmission statistics\n",
    "lsl_packet = np.array([i \n",
    "          for i, c in enumerate([len (x) for x in times_of_samples]) \n",
    "          for n in range(c)]\n",
    "        )\n",
    "\n",
    "# make a lsl timestamp signal (essential later on)\n",
    "lsl_time = np.concatenate(times_of_samples) - times_of_samples[0][0]\n",
    "\n",
    "# We add this information to the raw.\n",
    "info_lslinfo = mne.create_info(('lsl_time', 'lsl_packet'), sampling_freq, ('misc','misc'))\n",
    "raw_lslinfo = mne.io.RawArray((lsl_time, lsl_packet), info_lslinfo)\n",
    "raw_ec.add_channels([raw_lslinfo])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Inspect the ec data\n",
    "- scroll through the data see if the EEG signal is what you expect\n",
    "- mark bad channels (that cannot be rescued)\n",
    "- mark bad segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a simple display filter\n",
    "raw_ec_copy = raw_ec.copy()\n",
    "raw_ec_copy.filter(1, 35).plot(title='eyes closed');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-01/ses-01/eeg/sub-01_ses-01_task-ec_run-13.fif\n"
     ]
    }
   ],
   "source": [
    "# check bad channels in eo, compare to ec\n",
    "if not raw_ec_copy.info['bads'] == raw_eo.info['bads']:\n",
    "    # raise Exception('Bad Channels are not the same between the two datasets - fix this first')\n",
    "    all_bads = list(set(raw_ec_copy.info['bads'] + raw_eo.info['bads']))\n",
    "    raw_eo.info['bads'] = all_bads\n",
    "    raw_ec.info['bads'] = all_bads\n",
    "    \n",
    "    # save raw_eo again\n",
    "    raw_eo.save(fname_raw_eo_run, overwrite=True)\n",
    "    print('saved AGAIN: ' + fname_raw_eo_run)\n",
    "\n",
    "# - save this raw data to disk (filename taken care of)\n",
    "\n",
    "raw_ec.set_annotations(raw_ec_copy.annotations)\n",
    "raw_ec.info['bads'] = raw_ec_copy.info['bads']\n",
    "\n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'ec', run)\n",
    "fname_raw_ec_run = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "raw_ec.save(fname_raw_ec_run, overwrite=True)\n",
    "    \n",
    "print('saved: ' + fname_raw_ec_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Preprocessing Calibations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Calculate the Eyeblink Spatial Rejection\n",
    "- this will automatically read in the EO data  (run1)\n",
    "- and run the ICA analysis\n",
    "- your job is to select the component most resembling ocular artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply filter: 3 to 45\n",
      "Dropped 132 outliers\n",
      "ICA/CSP time elapsed = 4.806611061096191s\n",
      "Table drawing time elapsed = 4.550941467285156s\n",
      "Created an ICA Spatial Filter\n",
      "<pynfb.signal_processing.filters.SpatialRejection object at 0x7f063f6ef438>\n"
     ]
    }
   ],
   "source": [
    "bad_channel_mask = [ch not in raw_eo.info['bads'] for ch in raw_eo.ch_names[:-2]]\n",
    "\n",
    "# apply the annotations and bad channel mask to get a matrix of 'good' data:\n",
    "clean_raw = no_bad_channels(\n",
    "    no_bad_samples(\n",
    "        raw_eo.copy().filter(3, 40, picks='eeg').notch_filter((25, 31.25, 50))\n",
    "    )\n",
    ")\n",
    "\n",
    "# run the ICA analysis -- the ICA analysis already applies a 3-40 Hz filter!!\n",
    "from pynfb.protocols.ssd.topomap_selector_ica import ICADialog\n",
    "\n",
    "ica_rejection, _, _, ica_unmixing_matrix, _, _ = ICADialog.get_rejection(\n",
    "    clean_raw.get_data(picks='eeg').T, \n",
    "    clean_raw.ch_names[:-2],  # do not consider the last 2 channels in this calculation\n",
    "    clean_raw.info['sfreq'],\n",
    "    decomposition=None\n",
    ")\n",
    "\n",
    "bad_channel_mask = [ch not in raw_eo.info['bads'] for ch in raw_eo.ch_names[:-2]]\n",
    "ica_rejection = ica_rejection.expand_by_mask(bad_channel_mask)\n",
    "\n",
    "print('Created an ICA Spatial Filter')\n",
    "print(ica_rejection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Save the eyeblink Rejection\n",
    "- close all open windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-01/ses-01/eeg/sub-01_ses-01_ica-ocular-rejection_run-13.pkl\n"
     ]
    }
   ],
   "source": [
    "# we save the ICA for ocular rejection:\n",
    "# save the ICA rejection \n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_ica-ocular-rejection_run-{:02d}.pkl'.format(sub, ses, run)\n",
    "fname_ica_ocular_rejection = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "with open(fname_ica_ocular_rejection, 'wb') as f: pickle.dump(ica_rejection, f)\n",
    "print('saved: ' + fname_ica_ocular_rejection)\n",
    "\n",
    "# save it also as a .txt matrix (for matlab)\n",
    "np.savetxt(re.sub('.pkl$','.txt', fname_ica_ocular_rejection), ica_rejection.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Check the eyeblink Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot before ICA\n",
    "ica_compare = no_bad_samples(raw_eo.copy().filter(1, None).notch_filter((25, 31.25, 50)))\n",
    "ica_compare.drop_channels(('lsl_time','lsl_packet'))\n",
    "\n",
    "# plot after ICA\n",
    "after_ica = mne.io.RawArray(ica_rejection.apply(ica_compare.get_data().T).T, \n",
    "        mne.create_info(['ica-{}'.format(n) for n in ica_compare.ch_names], \n",
    "                        ica_compare.info['sfreq'],\n",
    "                        ['eeg' for i in range(len(ica_compare.ch_names))],\n",
    "                        None)\n",
    "               )\n",
    "after_ica.set_annotations(raw_eo.annotations)\n",
    "\n",
    "after_ica.filter(1, None).notch_filter((25, 31.25, 50))\n",
    "\n",
    "ica_compare.add_channels([after_ica])\n",
    "ica_compare.plot(n_channels=32);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Handle Alpha Power Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply filter: 3 to 45\n",
      "Dropped 242 outliers\n",
      "ICA/CSP time elapsed = 0.014914751052856445s\n",
      "Table drawing time elapsed = 6.100223779678345s\n"
     ]
    }
   ],
   "source": [
    "# we concatenate the 'eo' and the 'ec' data --> make eoec dataset\n",
    "clean_raw_eo = no_bad_channels(no_bad_samples(raw_eo.copy().filter(3, None)))\n",
    "clean_raw_ec = no_bad_channels(no_bad_samples(raw_ec.copy().filter(3, None)))\n",
    "\n",
    "eoec = mne.concatenate_raws([clean_raw_eo.copy(), clean_raw_ec])\n",
    "eoec.filter(None, 40).notch_filter((25, 31.25, 50)).drop_channels(('lsl_time','lsl_packet'))\n",
    "eoec.crop(tmin=1, tmax=eoec.times[-1]-1) # crop it to remove edge FX\n",
    "\n",
    "\n",
    "# Apply the eye blink rejection\n",
    "bad_channel_mask = [ch not in eoec.info['bads'] for ch in eoec.ch_names]\n",
    "shrunk_ica_rejection = ica_rejection.shrink_by_mask(bad_channel_mask)\n",
    "\n",
    "# make the dataset - WITH ICA applied --> make eoec_ica_applied dataset\n",
    "eoec_ica_applied = mne.io.RawArray(\n",
    "    shrunk_ica_rejection.apply(eoec.get_data().T).T, \n",
    "    mne.create_info(['ica-{}'.format(n) for n in eoec.ch_names],\n",
    "                        eoec.info['sfreq'],\n",
    "                        ['eeg' for i in range(len(eoec.ch_names))],\n",
    "                        None)\n",
    "    )\n",
    "eoec_ica_applied.set_annotations(eoec.annotations).filter(3, 40)\n",
    "\n",
    "\n",
    "# make a vector: 0 for 'eyes open'; 1 for 'eye closed'\n",
    "total_samples = eoec.last_samp - eoec.first_samp + 1\n",
    "samples_in_eo = clean_raw_eo.last_samp - eoec.first_samp + 1\n",
    "samples_in_ec = total_samples - samples_in_eo\n",
    "labels_for_csp = np.hstack((np.zeros(samples_in_eo), np.ones(samples_in_ec)))\n",
    "\n",
    "# do the Alpha Power calculation; bring up the GUI to select\n",
    "csp_rejection, filter, topography, _, bandpass, to_all = ICADialog.get_rejection(\n",
    "    eoec_ica_applied.get_data().T,\n",
    "    eoec.ch_names, \n",
    "    eoec_ica_applied.info['sfreq'],\n",
    "    mode='csp', \n",
    "    _stimulus_split=False,\n",
    "    labels=labels_for_csp, # will convert to 0-1 vector for each sample in x\n",
    "    marks=None)\n",
    "\n",
    "bad_channel_mask = [ch not in raw_eo.info['bads'] for ch in raw_eo.ch_names[:-2]]\n",
    "csp_rejection = csp_rejection.expand_by_mask(bad_channel_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Save the Alpha Power Rejection\n",
    "- close all open windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-01/ses-01/eeg/sub-01_ses-01_csp-alpha-rejection_run-13.pkl\n"
     ]
    }
   ],
   "source": [
    "# we save the ICA for ocular rejection:\n",
    "# save the ICA rejection \n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_csp-alpha-rejection_run-{:02d}.pkl'.format(sub, ses, run)\n",
    "fname_csp_alpha_rejection = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "with open(fname_csp_alpha_rejection, 'wb') as f: pickle.dump(csp_rejection, f)\n",
    "print('saved: ' + fname_csp_alpha_rejection)\n",
    "\n",
    "# save it also as .txt matrix (for matlab)\n",
    "np.savetxt(re.sub('.pkl$','.txt', fname_csp_alpha_rejection), csp_rejection.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Inspect Alpha Power Rejection\n",
    "- Check the time traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_channel_mask = [ch not in raw_eo.info['bads'] for ch in eoec_ica_applied.ch_names]\n",
    "\n",
    "shrunk_csp_rejection = csp_rejection.shrink_by_mask(bad_channel_mask)\n",
    "\n",
    "eoec_csp_and_ica_applied = mne.io.RawArray(\n",
    "    shrunk_csp_rejection.apply(eoec_ica_applied.get_data().T).T,\n",
    "    mne.create_info(['scp-{}'.format(n) for n in eoec_ica_applied.ch_names], \n",
    "                        eoec_ica_applied.info['sfreq'],\n",
    "                        ['eeg' for ch in eoec_ica_applied.ch_names], \n",
    "                        None,\n",
    "               )\n",
    ")\n",
    "\n",
    "eoec_csp_and_ica_applied.set_annotations(eoec_ica_applied.annotations).filter(3, 40)\n",
    "\n",
    "# make one big trace with ica an csp -channels. Future could be to plot - in trace - what I extracted..\n",
    "# this should be Unity_matrix - csp_removal_matrix.\n",
    "ica_csp_compare=eoec.copy()\n",
    "ica_csp_compare.add_channels([eoec_ica_applied, eoec_csp_and_ica_applied])\n",
    "ica_csp_compare.plot(title='After ICA and CSP', n_channels=3*len(eoec.ch_names));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Check the power spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-01/ses-01/eeg/sub-01_ses-01_run-13_spectra.jpg\n"
     ]
    }
   ],
   "source": [
    "# Plot/compare the power spectra:\n",
    "eoec_ica_applied_copy = eoec_ica_applied.copy(); \n",
    "eoec_ica_applied_copy.info=eoec.info\n",
    "eoec_csp_and_ica_applied_copy = eoec_csp_and_ica_applied.copy(); \n",
    "eoec_csp_and_ica_applied_copy.info=eoec.info\n",
    "fh=plt.figure(figsize=(12, 5))\n",
    "ah1=plt.subplot(131); eoec.plot_psd(ax=ah1); \n",
    "ah2=plt.subplot(132); eoec_ica_applied_copy.plot_psd(ax=ah2); ah2.set_ylim(ah1.get_ylim())\n",
    "ah3=plt.subplot(133); eoec_csp_and_ica_applied_copy.plot_psd(ax=ah3); ah3.set_ylim(ah1.get_ylim())\n",
    "ah1.set_title('No Spatial Filter')\n",
    "ah2.set_title('ICA Filter (eyeblinks gone)')\n",
    "ah3.set_title('CSP Filter (alpha gone) + ICA')\n",
    "spectra_plot_fname = this_raw_fname = 'sub-{:02d}_ses-{:02d}_run-{:02d}_spectra.jpg'.format(sub, ses, run)\n",
    "fh.savefig(os.path.join(this_save_dir, spectra_plot_fname))\n",
    "print('saved: {}'.format(os.path.join(this_save_dir, spectra_plot_fname)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Talk to Stimulus Computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Create Simulus Handler, I\n",
    "- connect via WiFi\n",
    "    - ip address = 10.42.0.1\n",
    "    - host = stim-pc\n",
    "    - password = PASSWORD, OR 12345678\n",
    "- ideally this should already be up and running, so you can skip over these more fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callpyff import bcinetwork, bcixml\n",
    "bcinet = bcinetwork.BciNetwork('20.100.0.2', bcinetwork.FC_PORT, bcinetwork.GUI_PORT, 'bcixml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Start Stimulus Marker Receiver\n",
    " - this will listen on UDP port 6500 for any incoming markers\n",
    " - This is to convert signals from the Presentation into annotations\n",
    " - grab markers in the NF loop with: `while not marker_queue.empty():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netifaces\n",
    "#default_interface=netifaces.gateways()['default'][netifaces.AF_INET][1]\n",
    "#ip_address=netifaces.ifaddresses(default_interface)[netifaces.AF_INET][0]['addr']\n",
    "ip_address='20.100.0.1'\n",
    "port = 6500\n",
    "#print('Tell Stimulus computer to send markers to our interface {} with ip={} and port={}'.format(default_interface, ip_address, port))\n",
    "\n",
    "bcinet.send_signal(bcixml.BciSignal({'EVENT_destip': ip_address},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'EVENT_destport': port},None, bcixml.INTERACTION_SIGNAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening on :6500\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def my_marker_server(marker_queue, PORT):\n",
    "    # simple markers server.\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    sock.bind(('', PORT))\n",
    "    print(\"Listening on {}:{}\".format('', PORT))\n",
    "    while True:\n",
    "        data = sock.recv(1024)\n",
    "        if data == b'stop_server':\n",
    "            break\n",
    "        marker_queue.put(data)\n",
    "\n",
    "    print('finished')\n",
    "    sock.close()\n",
    "    \n",
    "def send_stop_it(port):\n",
    "    sock=socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    sock.sendto(b'stop_server', ('localhost', port))\n",
    "    sock.close()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "if not 'marker_queue' in locals():\n",
    "    marker_queue = Queue()\n",
    "if 'my_udp_server' in locals():\n",
    "    # we stop the marker server\n",
    "    send_stop_it(port)\n",
    "    try:\n",
    "        # and we try to join it\n",
    "        my_udp_server.join()\n",
    "    except:\n",
    "        del(my_udp_server)\n",
    "        \n",
    "my_udp_server = Process(target=my_marker_server, args=(marker_queue, port))\n",
    "my_udp_server.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Define RT Signal analysis flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.1 RT Signal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynfb.signal_processing.filters import (FilterSequence, \n",
    "                                             CFIRBandEnvelopeDetector, \n",
    "                                             ExponentialSmoother,\n",
    "                                             SpatialFilter,\n",
    "                                             ButterFilter,\n",
    "                                             ButterBandEnvelopeDetector,\n",
    "                                             ScalarButterFilter,\n",
    "                                             MASmoother,\n",
    "                                             FFTBandEnvelopeDetector,\n",
    "                                             NotchFilter\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Filter Sequence for NF\n",
    "\n",
    "# Which channel do we select for the EEG - we do C3.\n",
    "rt_eeg_channels = ['C4']\n",
    "rt_eeg_channels_mask = np.where([ch in rt_eeg_channels for ch in channel_names], 1, 0)/len(rt_eeg_channels)\n",
    "eeg_channel_select = SpatialFilter(rt_eeg_channels_mask)\n",
    "highpass_filter = ScalarButterFilter([3, None], sampling_freq)\n",
    "butter_filter = ScalarButterFilter([12, 15], sampling_freq)\n",
    "\n",
    "eeg_notch_50 = NotchFilter(50, sampling_freq, len(channel_names))\n",
    "# eeg_notch_25 = NotchFilter(25, sampling_freq, len(channel_names))\n",
    "\n",
    "preprocess_filters_eeg = FilterSequence([\n",
    "    ica_rejection,\n",
    "    csp_rejection,\n",
    "    eeg_channel_select,\n",
    "    highpass_filter\n",
    "    \n",
    "])\n",
    "envelope_filter_eeg = CFIRBandEnvelopeDetector([12, 15], sampling_freq, MASmoother(round(sampling_freq/5)))\n",
    "butter_visualization_eeg = ButterFilter([12, 15], sampling_freq, 1)\n",
    "\n",
    "\n",
    "# Processing of the EMG - this is basically our second channel...\n",
    "rt_emg_channels = ['T3','T4','Fp1','Fp2']\n",
    "# rt_emg_channels = ['T7','T8','TP9','TP10']\n",
    "rt_emg_channels_mask = np.where([ch in rt_emg_channels for ch in channel_names], 1, 0)/len(rt_emg_channels)\n",
    "emg_channel_select = SpatialFilter(rt_emg_channels_mask)\n",
    "\n",
    "preprocess_filters_emg = FilterSequence([\n",
    "    eeg_notch_50,\n",
    "    emg_channel_select,\n",
    "])\n",
    "\n",
    "envelope_filter_emg = ButterBandEnvelopeDetector([30, 62], sampling_freq, MASmoother(round(sampling_freq/5)))\n",
    "butter_visualization_emg = ButterFilter([30, 60], sampling_freq, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Appy Rt Filters on Calibration Data\n",
    "This will:\n",
    "- apply the RT filters to our existing data\n",
    "- calibrate the signaltracking objects (that send stuff to the Stimulus) properly\n",
    "- set all the scalings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eoec_rawraw = mne.concatenate_raws([raw_eo.copy(), raw_ec])\n",
    "# in order to determine threshold for EEG and EMG -- use ONLY (!) the raw_eo!\n",
    "\n",
    "\n",
    "# eoec_rawraw = raw_eo.copy().drop_channels(('lsl_time','lsl_packet'))\n",
    "eoec_rawraw = mne.concatenate_raws([raw_eo.copy(), raw_ec]).drop_channels(('lsl_time','lsl_packet'))\n",
    "\n",
    "# eoec_rawraw.plot();\n",
    "\n",
    "preprocessed_eeg = preprocess_filters_eeg.apply(eoec_rawraw.get_data().T)*1E6\n",
    "preprocessed_emg = preprocess_filters_emg.apply(eoec_rawraw.get_data().T)*1E6\n",
    "\n",
    "eeg_vec = envelope_filter_eeg.apply(preprocessed_eeg)\n",
    "emg_vec = envelope_filter_emg.apply(preprocessed_emg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-01/ses-01/eeg/sub-01_ses-01_calibration_envelopes_run-13.pkl\n",
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-01/ses-01/eeg/sub-01_ses-01_calibration_envelopes_run-13.mat\n"
     ]
    }
   ],
   "source": [
    "# save the eeg_vec and emg_vec also as .pkl and .mat file\n",
    "calibration_envelopes = {'eeg_vec':eeg_vec, 'emg_vec':emg_vec, 'preprocessed_eeg': preprocessed_eeg}\n",
    "\n",
    "# save it\n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_calibration_envelopes_run-{:02d}.pkl'.format(sub, ses, run)\n",
    "fname_calibration_envelopes = os.path.join(this_save_dir, this_raw_fname)\n",
    "with open(fname_calibration_envelopes, 'wb') as f: pickle.dump(calibration_envelopes, f)\n",
    "print('saved: ' + fname_calibration_envelopes)\n",
    "spio.savemat(re.sub('.pkl$','.mat', fname_calibration_envelopes), calibration_envelopes)\n",
    "print('saved: ' + re.sub('.pkl$','.mat', fname_calibration_envelopes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Use calibration data to set thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eeg: mean 8.654, median 4.176, mode 2.973: \n",
      "emg: mean 3.841, median 3.061, mode 0.000: \n",
      "2.00 minutes of data\n",
      "mode_thr_eeg = 2.042\n",
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-01/ses-01/eeg/sub-01_ses-01_calibration_parameters_run-13.pkl\n",
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-01/ses-01/eeg/sub-01_ses-01_calibration_parameters_run-13.mat\n"
     ]
    }
   ],
   "source": [
    "butter_filter = ScalarButterFilter([12, 15], sampling_freq)\n",
    "\n",
    "mode_eeg = find_mode(butter_filter.apply(preprocessed_eeg), make_plot=True)\n",
    "median_eeg = np.median(eeg_vec)\n",
    "mean_eeg = np.mean(eeg_vec)\n",
    "print('eeg: mean {:2.3f}, median {:2.3f}, mode {:2.3f}: '.format(mean_eeg, median_eeg, mode_eeg))\n",
    "mode_fig = plt.gcf()\n",
    "\n",
    "mode_emg = None # we won't bother detecting mode of EMG with poor quality EMG\n",
    "median_emg = np.median(emg_vec)\n",
    "mean_emg = np.mean(emg_vec)\n",
    "print('emg: mean {:2.3f}, median {:2.3f}, mode {:2.3f}: '.format(mean_emg, median_emg, 0))\n",
    "\n",
    "print('{:3.2f} minutes of data'.format(len(preprocessed_eeg)/sampling_freq/60))\n",
    "\n",
    "# determine the threshold for 90 bursts!\n",
    "mode_thr_eeg, _, _ = determine_optimal_threshold(butter_filter.apply(preprocessed_eeg), sampling_freq, 0.2, 90, make_plot=True)\n",
    "print('mode_thr_eeg = {:2.3f}'.format(mode_thr_eeg))\n",
    "mode_thr_fig = plt.gcf()\n",
    "\n",
    "median_thr_emg = 10.\n",
    "\n",
    "EEG_THR = mode_thr_eeg * mode_eeg\n",
    "EEG_DUR = 0.20\n",
    "EEG_STSCALING = 10 * mode_eeg\n",
    "\n",
    "EMG_THR = median_thr_emg * median_emg\n",
    "EMG_DUR = 0.25\n",
    "EMG_STSCALING = 2 * EMG_THR\n",
    "\n",
    "\n",
    "\n",
    "calibration_parameters = {'mode_eeg':mode_eeg,\n",
    "                          'mode_emg':0.,\n",
    "                          'mode_thr_eeg':mode_thr_eeg,\n",
    "                          'median_thr_emg':median_thr_emg,\n",
    "                          'EEG_THR':EEG_THR, \n",
    "                          'EEG_DUR':EEG_DUR, \n",
    "                          'EEG_STSCALING': EEG_STSCALING, \n",
    "                          'EMG_THR':EMG_THR,\n",
    "                          'EMG_DUR':EMG_DUR,\n",
    "                          'EMG_STSCALING':EMG_STSCALING\n",
    "                         }\n",
    "\n",
    "# save those too\n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_calibration_parameters_run-{:02d}.pkl'.format(sub, ses, run)\n",
    "fname_calibration_parameters = os.path.join(this_save_dir, this_raw_fname)\n",
    "with open(fname_calibration_parameters, 'wb') as f: pickle.dump(calibration_parameters, f)\n",
    "print('saved: ' + fname_calibration_parameters)\n",
    "spio.savemat(re.sub('.pkl$','.mat', fname_calibration_parameters), calibration_parameters)\n",
    "print('saved: ' + re.sub('.pkl$','.mat', fname_calibration_parameters))\n",
    "\n",
    "mode_fig.savefig(os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_mode_run-{:02d}.png'.format(sub, ses, run)))\n",
    "mode_thr_fig.savefig(os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_threshold_run-{:02d}.png'.format(sub, ses, run)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Create Stimulus Handlers, II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr: 6.07, dur: 0.20\n",
      "bcinet is passed on\n",
      "thr: 30.61, dur: 0.25\n",
      "bcinet is passed on\n"
     ]
    }
   ],
   "source": [
    "# these objects can send variables over to the stimulus\n",
    "# parameters to convert the filtered EEG signal to the stimulus\n",
    "# the following parameter should come out of the EEG data, as our initial threshold to use:\n",
    "# global_std_band=5\n",
    "\n",
    "from nftools.nftools import signaltracking\n",
    "track_for_eeg_stimuli = signaltracking.sending_to_nfstim(\n",
    "    sampling_freq,\n",
    "    thr=EEG_THR, \n",
    "    dur=EEG_DUR, \n",
    "    feedback_type='eeg', \n",
    "    max4audio=1.2, \n",
    "    bcinet=bcinet, \n",
    "    st_scaling=10,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# parameters to convert the filtered EMG signal to the stimulus\n",
    "track_for_emg_stimuli = signaltracking.sending_to_nfstim(\n",
    "    sampling_freq,\n",
    "    thr=EMG_THR, \n",
    "    dur=EMG_DUR, \n",
    "    feedback_type='emg', \n",
    "    bcinet=bcinet, \n",
    "    st_scaling=EMG_STSCALING,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 The Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Start the NF Stimulation\n",
    " - you still have to press <ENTER> to actually start the stimulus, but Not Yet!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TestD2', 'MovingRhomb', 'EOEC', 'LibetClock', 'BrainWaveTraining_II', 'TobiQLAdapter', 'EyetrackerRawdata', 'EyetrackerFeedback', 'HexoSpeller', 'P300_Rectangle', 'ERPHex', 'BrainWaveTraining', 'StopVigilanceTask', 'FeedbackCursorArrow', 'TrivialPong', 'CheckerboardVEP', 'HexoSpellerVE', 'BoringClock', 'nback_verbal', 'VisualOddball', 'BrainPong', 'CakeSpellerVE', 'MovingRhombGL', 'RestingState', 'NFBasicThermometer', 'RSVPSpeller', 'EEGfMRILocalizer', 'Oddball', 'Lesson01b', 'GoalKeeper', 'CenterSpellerVE', 'MultiVisualOddball', 'StroopFeedback', 'ERPMatrix', 'Lesson04', 'Lesson05', 'Lesson06', 'Lesson01', 'Lesson02', 'Lesson03', 'VisualOddballVE']\n"
     ]
    }
   ],
   "source": [
    "print(bcinet.getAvailableFeedbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcinet.send_init('BrainWaveTraining_II')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the stimulus about the monitor\n",
    "bcinet.send_signal(bcixml.BciSignal({'EX_TESTNFNOISE': False},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'EX_PR_SLEEPTIME': 0.005},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'MONITOR_PIXWIDTH': 1366},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'MONITOR_PIXHEIGHT': 768},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'MONITOR_FULLSCR': True},None, bcixml.INTERACTION_SIGNAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcinet.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 make the windows for plotting\n",
    "- move them somewhere convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our UI 'Experience' -- it can consist of 3 separate, movable windows (for now)\n",
    "# same window as before + 2 other windows - 1 for interaction with stim/thresholds; 1 for looking\n",
    "# at the analysis itself.\n",
    "\n",
    "w_acquire = guis.AcquireData(sampling_freq, channel_names)\n",
    "w_interaction = guis.NFChangeThresholds(track_for_eeg_stimuli, track_for_emg_stimuli)\n",
    "w_eeganalysis = guis.AnalyzeData(sampling_freq, ['EEG','env','thr','vmarker','amarker'],track_for_eeg_stimuli)\n",
    "w_emganalysis = guis.AnalyzeData(sampling_freq, ['EMG','env','thr','vmarker','amarker'],track_for_emg_stimuli)\n",
    "\n",
    "from PyQt5 import QtWidgets, QtGui\n",
    "class AllInOneWindow(QtWidgets.QWidget):\n",
    "    def __init__(self):\n",
    "        super(AllInOneWindow, self).__init__()\n",
    "        layout = QtWidgets.QGridLayout()\n",
    "        layout.addWidget(w_acquire, 1, 1, 1, 1)\n",
    "        layout.addWidget(w_interaction, 2, 1, 1, 1)\n",
    "        layout.addWidget(w_eeganalysis, 1, 2, 1, 1)\n",
    "        layout.addWidget(w_emganalysis, 2, 2, 1, 1)\n",
    "        \n",
    "        # remove the checkboxes of all except the EEG analysis window:\n",
    "        w_acquire.datawidget.plot_check_box.setChecked(False)\n",
    "        w_emganalysis.plot_check_box.setChecked(False)\n",
    "        \n",
    "        self.setLayout(layout)\n",
    "        self.show()\n",
    "\n",
    "\n",
    "# can we combine everything into 1 Big Gui -- YES\n",
    "big_window = AllInOneWindow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Staring the NF Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n"
     ]
    }
   ],
   "source": [
    "# clear EEG Data buffer\n",
    "data_inlet.pull_chunk()\n",
    "while data_inlet.samples_available(): data_inlet.pull_chunk() \n",
    "\n",
    "# containers for data collection: \n",
    "\n",
    "data_nf = dynarray.DynamicArray((None, len(channel_names))); times_of_samples = []\n",
    "data_analysis_eeg = dynarray.DynamicArray((None, 5))\n",
    "data_analysis_emg = dynarray.DynamicArray((None, 5))\n",
    "\n",
    "# collect markers from the Stimulation:\n",
    "stim_Annotations = mne.Annotations(0, 0, 'Start NF Loop')\n",
    "\n",
    "# collect markers from the interaction GUI:\n",
    "w_interaction.GUI_Annotations.append(time.time()-w_interaction.begin_time, 0, 'startloop')\n",
    "\n",
    "# start the loop\n",
    "w_acquire.RUNLOOP=True\n",
    "acquisition_start = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "t_begin=time.time()\n",
    "while w_acquire.RUNLOOP:\n",
    "    \n",
    "    # wait 1 msec\n",
    "    time.sleep(0.001)\n",
    "    \n",
    "    \n",
    "    # check if the presentation gave any markers, and collect them\n",
    "    while not marker_queue.empty():\n",
    "        stim_Annotations.append(time.time()-t_begin, 0, marker_queue.get().decode())\n",
    "    \n",
    "    # handle the incoming data (if any):\n",
    "    if not data_inlet.samples_available(): \n",
    "        w_acquire.update(None)\n",
    "    else:\n",
    "        chunk_data, chunk_times = data_inlet.pull_chunk() # grab from LSL\n",
    "        \n",
    "        # how to deal with missing data -- do we need to insert any placeholder values?\n",
    "        \n",
    "        \n",
    "\n",
    "        # update signal window so we can see raw signals\n",
    "        w_acquire.update(chunk_data) \n",
    "        \n",
    "        # store the raw data (and times)\n",
    "        times_of_samples.append(chunk_times)\n",
    "        data_nf.extend(chunk_data)\n",
    "\n",
    "        \n",
    "        #\n",
    "        # EEG Signal processing\n",
    "        #\n",
    "        \n",
    "        # apply spatial and temporal filters to the raw signal; for EEG and EMG:\n",
    "        preprocessed_eeg = preprocess_filters_eeg.apply(chunk_data)        \n",
    "        envelope_eeg = envelope_filter_eeg.apply(preprocessed_eeg)\n",
    "\n",
    "\n",
    "        # check if it's above threshold or not; send markers to stimulus computer\n",
    "        visual_markers_eeg, audio_markers_eeg = track_for_eeg_stimuli.check_above_threshold(envelope_eeg)  \n",
    "        # send the signal  to stimulus computer, too\n",
    "        track_for_eeg_stimuli.send_data_signal(envelope_eeg) \n",
    "        \n",
    "\n",
    "        # visualize the processing steps\n",
    "        # analysis_names_eeg = ('EEG', 'env', 'thr', 'vmarker', 'amarker')\n",
    "        analysis_data_eeg = np.vstack((\n",
    "            np.abs(butter_visualization_eeg.apply(preprocessed_eeg[:, None])[:, 0]),\n",
    "            envelope_eeg,\n",
    "            track_for_eeg_stimuli.thr * np.ones(preprocessed_eeg.shape),\n",
    "            visual_markers_eeg,\n",
    "            audio_markers_eeg,\n",
    "        )).T\n",
    "        \n",
    "        data_analysis_eeg.extend(analysis_data_eeg)  # store it for later conversion\n",
    "        w_eeganalysis.update(analysis_data_eeg) # update analysis window\n",
    "\n",
    "        \n",
    "        #\n",
    "        # EMG Signal Processing\n",
    "        #\n",
    "        \n",
    "        # do the same for the EMG NF, too:\n",
    "        # apply spatial and temporal filters to the raw signal; for EEG and EMG:\n",
    "        preprocessed_emg = preprocess_filters_emg.apply(chunk_data)\n",
    "        envelope_emg = envelope_filter_emg.apply(preprocessed_emg)\n",
    "        \n",
    "        # check if it's above threshold or not; send markers to stimulus computer\n",
    "        visual_markers_emg, audio_markers_emg = track_for_emg_stimuli.check_above_threshold(envelope_emg) \n",
    "        # send the signal  to stimulus computer, too\n",
    "        track_for_emg_stimuli.send_data_signal(envelope_emg) \n",
    "\n",
    "        # visualize the processing steps\n",
    "        # analysis_names_emg = ('EMG', 'env', 'thr', 'vmarker', 'amarker')\n",
    "        analysis_data_emg = np.vstack((\n",
    "            np.abs(butter_visualization_emg.apply(preprocessed_emg[:, None])[:, 0]),\n",
    "            envelope_emg,\n",
    "            track_for_emg_stimuli.thr * np.ones(preprocessed_emg.shape),\n",
    "            visual_markers_emg,\n",
    "            audio_markers_emg,\n",
    "        )).T\n",
    "        \n",
    "        data_analysis_emg.extend(analysis_data_emg)  # store it for later conversion\n",
    "        w_emganalysis.update(analysis_data_emg) # update analysis window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Now the participant will do the training\n",
    "- start it up on the NF Laptop now\n",
    "- wait until the training is done\n",
    "- then stop the loop as normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 make the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RawArray  |  None, n_channels x n_times : 18 x 62778 (502.2 sec), ~8.7 MB, data loaded>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we make the MNE data file from it        \n",
    "raw_nftraining = mne.io.RawArray(np.transpose(data_nf)*1E-6,\n",
    "                        mne.create_info(channel_names, \n",
    "                                    sampling_freq, \n",
    "                                    detect_channel_types(channel_names), \n",
    "                                    'standard_1020'\n",
    "                           )\n",
    "                     )\n",
    "\n",
    "# attribute a number (1, 2, 3, ... etcetc) to each incoming packet of samples\n",
    "lsl_packet = np.array([i \n",
    "          for i, c in enumerate([len (x) for x in times_of_samples]) \n",
    "          for n in range(c)]\n",
    "        )\n",
    "\n",
    "# make a lsl timestamp signal (essential later on)\n",
    "lsl_time = np.concatenate(times_of_samples) - times_of_samples[0][0]\n",
    "\n",
    "# We add this information to the raw.\n",
    "info_lslinfo = mne.create_info(('lsl_time', 'lsl_packet'), sampling_freq, ('misc','misc'))\n",
    "raw_lslinfo = mne.io.RawArray((lsl_time, lsl_packet), info_lslinfo)\n",
    "raw_nftraining.add_channels([raw_lslinfo])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 annotation(s) of the EEG\n",
    "The following sections are all intended to annotate the EEG with:\n",
    "- when does the trial start and end?\n",
    "- when did the presentation give an audio signal?\n",
    "- when did the presentation give a video signal?\n",
    "- when did you (as the experimenter) press any button on the GUI? What value?\n",
    "- when did the LSL datastream break up (bad segment)?\n",
    "\n",
    "You can just shift-Enter through them all\n",
    "\n",
    "All of these items will be detected from various annotation variables and\n",
    "data accumulated during the training run. All raw data will also be saved.\n",
    "They will all be converted to annotations that will be put into the EEG trace.\n",
    "One figure will be created to illustrate (and double check) the datastream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle annotations, since w_interaction starts before data acquisition\n",
    "annots = w_interaction.GUI_Annotations.copy()\n",
    "tdelta = datetime.strptime(acquisition_start,'%Y-%m-%d %H:%M:%S.%f').timestamp() - annots.orig_time\n",
    "annots.onset -= tdelta\n",
    "annots.orig_time=None\n",
    "\n",
    "# grab from those the staring ones - change their times...\n",
    "starting_annots = annots[list(map(lambda d: re.match('start.*', d) is not None, annots.description))]\n",
    "starting_annots.onset = 0.1*np.ones(len(starting_annots))\n",
    "starting_annots.orig_time = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract\n",
    "to_process = {'irest':'11', 'brest':'12','erest':'13', \n",
    " 'itrain':'31', 'btrain':'32','etrain':'33', \n",
    " 'itransfer':'41', 'btransfer':'42','etransfer':'43',\n",
    " 'hit':'170', 'p':'171', 'emg':'230'}\n",
    "\n",
    "# bookkeeping on the annotations! - make things nice, near, easy to understand...\n",
    "new_annots = mne.Annotations(0, 0, ''); new_annots.delete(0)\n",
    "\n",
    "for key, value in to_process.items():\n",
    "    tmp_annots = stim_Annotations[[i for i, d in enumerate(stim_Annotations.description) if re.fullmatch(d, value)]]\n",
    "    \n",
    "    for item in tmp_annots:\n",
    "        # this will deal with train, transfer and rest blocks:\n",
    "        if item['description'] in ['11', '31', '41']:\n",
    "            # its an instruction\n",
    "            new_annots.append(item['onset'], 0, key)\n",
    "        if item['description'] in ['12', '32', '42']:\n",
    "            # it's the onset of a rest/training/transfer interval - where is the end?\n",
    "            to_find = '{}3'.format(item['description'][0])\n",
    "            # find all endings of current trans/train/rest:\n",
    "            all_endings = np.array([item['onset'] for i, item in enumerate(stim_Annotations) if item['description'] == to_find])\n",
    "            # find the one ending that we need:\n",
    "            this_duration = all_endings[next((i for i, j in enumerate(all_endings > item['onset']) if j), [])] - item['onset']\n",
    "            # if we find it, complete block: add annotation.\n",
    "            if this_duration:\n",
    "                new_annots.append(item['onset'], this_duration, key[1:])\n",
    "\n",
    "        # this deals with NF hits and misses:\n",
    "        if item['description'] in ['170','171']:\n",
    "            new_annots.append(item['onset'], 0, key)\n",
    "            \n",
    "        # this deal with the EMG:\n",
    "        if item['description'] in ['230']:\n",
    "            new_annots.append(item['onset']-0.5, 1.0, 'BAD_' + key)\n",
    "            \n",
    "# bad segment annotations:\n",
    "new_annots = mne.Annotations(0, 0, ''); new_annots.delete(0)\n",
    "            \n",
    "# these are all annotations so far - we're going to correct their onsets:            \n",
    "all_annotations = starting_annots + annots + new_annots\n",
    "\n",
    "# put the markers at the right spot by median filter of delay w.r.t. naive time:\n",
    "uncorrected_time = raw_nftraining.times\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "delay_model=median_filter(lsl_time - uncorrected_time, round(1.0*sampling_freq))\n",
    "corrected_time = uncorrected_time + delay_model\n",
    "\n",
    "# this seems a bad way to loop over the annotations - but it is\n",
    "# the only way to actually change the onset timings (normal looping - doesn't work!)\n",
    "for i in range(len(all_annotations)):\n",
    "    uncorrected_timestamp = all_annotations.onset[i]\n",
    "    corrected_index = np.argmin(abs(corrected_time - uncorrected_timestamp))\n",
    "    corrected_timestamp = corrected_index / sampling_freq\n",
    "    all_annotations.onset[i] = corrected_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f062db8a2b0>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(delay_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine segments that are bad, and annotate them too:\n",
    "wsize = round(0.5*sampling_freq)\n",
    "# to facilitate sliding window selection with indices\n",
    "sliding_inds = iter([range(s, s+wsize) for s in range(len(delay_model)-wsize)])\n",
    "# determine slope and intercept in sliding window\n",
    "slopes, intercepts = np.array([np.polyfit(inds, delay_model[inds], 1) for inds in sliding_inds]).T\n",
    "# the threshold (works well)\n",
    "thr = np.median(abs(slopes)) * 6.0 \n",
    "# true/false array of bad points -- but broaden it a little bit more (1 second)\n",
    "bad_points=np.convolve(slopes > thr, np.ones(round(sampling_freq), dtype='bool'), mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make our dictionary -> later will be converted towards annotation(s)\n",
    "bad_segments = []\n",
    "# a list of index items\n",
    "cur_prev_inds = iter([(s, s+1) for s in range(len(bad_points)-1)])\n",
    "for i, (cur, nxt) in enumerate(cur_prev_inds):\n",
    "    \n",
    "    if bad_points[nxt] and not bad_points[cur] or bad_points[cur] and i == 0:\n",
    "        # we need to add half length of window size for getting the real start\n",
    "        bad_segments.append({'start':i + round(wsize/2), 'duration':0})\n",
    "    if bad_points[nxt] and bad_points[cur]:\n",
    "        bad_segments[-1]['duration'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# insert these bad segments as annotations\n",
    "# no tdelay correction necessary -- we know the samples\n",
    "bad_segment_annots = mne.Annotations(0, 0, ''); bad_segment_annots.delete(0)\n",
    "for segment in bad_segments:\n",
    "    bad_segment_annots.append(segment['start']/sampling_freq, segment['duration']/sampling_freq, 'BAD_Transmission')\n",
    "\n",
    "all_annotations += bad_segment_annots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes few seconds\n",
    "fig=plt.figure(figsize=(15, 5))\n",
    "plt.plot(lsl_time-uncorrected_time)\n",
    "ylims = plt.ylim()\n",
    "plt.xlim(0, len(lsl_time))\n",
    "plt.xlabel('sample'); plt.ylabel('delay (s)')\n",
    "\n",
    "for segment in bad_segments:\n",
    "    plt.vlines(segment['start']+segment['duration']/2, *ylims)\n",
    "fname = 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}_datapackets.jpg'.format(sub, ses, 'raw-nftraining', run)\n",
    "plt.title(fname)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(this_save_dir, fname))\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Saving the data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-01/ses-01/eeg/sub-01_ses-01_task-raw-nftraining_run-13.fif\n"
     ]
    }
   ],
   "source": [
    "# saving the raw data - as of yet, NO annotations!\n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'raw-nftraining', run)\n",
    "fname_raw_nftraining_run = os.path.join(this_save_dir, this_raw_fname)\n",
    "raw_nftraining.set_annotations(all_annotations)\n",
    "raw_nftraining.save(fname_raw_nftraining_run, overwrite=True)\n",
    "print('saved: ' + fname_raw_nftraining_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-01/ses-01/eeg/sub-01_ses-01_task-rtanalyzed-nftraining-eeg_run-13.fif\n"
     ]
    }
   ],
   "source": [
    "# and we make the MNE data file from it\n",
    "analysis_names_eeg = ('EEG', 'env', 'thr', 'vmarker', 'amarker')\n",
    "rtanalyzed_nftraining_eeg = mne.io.RawArray(np.transpose(np.array(data_analysis_eeg) * [1E-6, 1E-6, 1E-6, 1, 1]),\n",
    "                        mne.create_info(analysis_names_eeg, \n",
    "                                    sampling_freq, \n",
    "                                    ['eeg','eeg','eeg','stim','stim'], \n",
    "                                    None)\n",
    "                       )\n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'rtanalyzed-nftraining-eeg', run)\n",
    "fname_rtanalyzed_nftraining_eeg = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "rtanalyzed_nftraining_eeg.set_annotations(all_annotations)\n",
    "rtanalyzed_nftraining_eeg.save(fname_rtanalyzed_nftraining_eeg, overwrite=True)\n",
    "print('saved: ' + fname_rtanalyzed_nftraining_eeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-01/ses-01/eeg/sub-01_ses-01_task-rtanalyzed-nftraining-emg_run-13.fif\n"
     ]
    }
   ],
   "source": [
    "# and we make the MNE data file from it\n",
    "analysis_names_emg = ('EMG', 'env', 'thr', 'vmarker', 'amarker')\n",
    "rtanalyzed_nftraining_emg = mne.io.RawArray(np.transpose(np.array(data_analysis_emg) * [1E-6, 1E-6, 1E-6, 1, 1]),\n",
    "                        mne.create_info(analysis_names_emg, \n",
    "                                    sampling_freq, \n",
    "                                    ['emg','emg','emg','stim','stim'], \n",
    "                                    None)\n",
    "                       )\n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'rtanalyzed-nftraining-emg', run)\n",
    "fname_rtanalyzed_nftraining_emg = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "\n",
    "rtanalyzed_nftraining_emg.set_annotations(all_annotations)\n",
    "rtanalyzed_nftraining_emg.save(fname_rtanalyzed_nftraining_emg, overwrite=True)\n",
    "print('saved: ' + fname_rtanalyzed_nftraining_emg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "The following is just to plot some of the EEG traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_nftraining.copy().filter(3, 40).notch_filter((25, 50)).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtanalyzed_nftraining_emg.plot();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
