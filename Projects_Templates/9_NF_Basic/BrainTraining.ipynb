{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Brain Training\n",
    "This is the notebook where everything regarding NF Training will come together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 modules and where to put files\n",
    "- double check all these parameters\n",
    "- change sub and sess according to the current participant and session (!)\n",
    "- double check the destination directory to be sure\n",
    "- don't use the Template Notebook (if you do so...), but copy/paste a notebook into a subject/session specific directory!\n",
    "- do not do this work just before a measurement. Have it prepared (and preferable tested!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import (most of the) modules that we need\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from multiprocessing import Process\n",
    "import numpy as np\n",
    "import pylsl\n",
    "from datetime import datetime\n",
    "from nftools import guis\n",
    "from nftools.mne import no_bad_samples, no_bad_channels, detect_channel_types\n",
    "from nftools.threshold import find_mode, contiguous_regions, detect_bursts, determine_optimal_threshold\n",
    "import mne\n",
    "import dynarray\n",
    "import pickle\n",
    "from scipy import io as spio\n",
    "import matplotlib.pyplot as plt\n",
    "from pynfb.protocols.ssd.topomap_selector_ica import ICADialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the qt event loop, disable warnings (they flood the screen)\n",
    "%matplotlib qt  \n",
    "# %gui qt\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore') \n",
    "mne.set_config('MNE_LOGGING_LEVEL', 'WARNING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Define sub, ion and run numbers\n",
    "- this will also load all the available data - if they've already been recorded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these according to which sub-sess we have:\n",
    "sub = 2\n",
    "ses = 14\n",
    "run = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  /home/johan/nf/rawdata/BrainTraining/bids/sub-02/ses-14/eeg  already exists\n",
      "found raw eyes open: \t\tsub-02_ses-14_task-eo_run-01.fif\n",
      "found raw eyes closed: \t\tsub-02_ses-14_task-ec_run-01.fif\n",
      "found ica eyeblink rejection:\tsub-02_ses-14_ica-ocular-rejection_run-01.pkl\n",
      "found csp alpha rejection: \tsub-02_ses-14_csp-alpha-rejection_run-01.pkl\n",
      "found calibration envelopes: \tsub-02_ses-14_calibration_envelopes_run-01.pkl\n",
      "found calibration parameters: \tsub-02_ses-14_calibration_parameters_run-01.pkl\n"
     ]
    }
   ],
   "source": [
    "# define where we put our stuff - prepare for BIDS Format Style\n",
    "home_dir = os.path.expanduser('~')\n",
    "save_dir = os.path.join(home_dir, 'nf/rawdata/BrainTraining/bids')\n",
    "\n",
    "this_save_dir = os.path.join(save_dir, 'sub-{:02d}'.format(sub), 'ses-{:02d}'.format(ses), 'eeg')\n",
    "\n",
    "# we should also ... MAKE this savedir! If it exists, we don't do anything.\n",
    "if not os.path.exists(this_save_dir):\n",
    "    os.makedirs(this_save_dir)\n",
    "    print(\"Directory \" , this_save_dir ,  \" Created \")\n",
    "else:    \n",
    "    print(\"Directory \" , this_save_dir ,  \" already exists\")   \n",
    "    \n",
    "    \n",
    "# expected files to be read/written:\n",
    "fname_raw_eo_run = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'eo', run))\n",
    "fname_raw_ec_run = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'ec', run))\n",
    "fname_ica_ocular_rejection = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_ica-ocular-rejection_run-{:02d}.pkl'.format(sub, ses, run))\n",
    "fname_csp_alpha_rejection = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_csp-alpha-rejection_run-{:02d}.pkl'.format(sub, ses, run))\n",
    "\n",
    "fname_calibration_envelopes = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_calibration_envelopes_run-{:02d}.pkl'.format(sub, ses, run))\n",
    "fname_calibration_parameters = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_calibration_parameters_run-{:02d}.pkl'.format(sub, ses, run))\n",
    "\n",
    "fname_raw_nftraining_run = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'raw-nftraining', run))\n",
    "fname_rtanalyzed_nftraining_eeg = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'rtanalyzed-nftraining-eeg', run))\n",
    "fname_rtanalyzed_nftraining_emg = os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'rtanalyzed-nftraining-emg', run))\n",
    "\n",
    "\n",
    "# if files exist, load them\n",
    "if os.path.exists(fname_raw_eo_run):\n",
    "    print('found raw eyes open: \\t\\t' + os.path.basename(fname_raw_eo_run))\n",
    "    raw_eo = mne.io.read_raw_fif(fname_raw_eo_run, preload=True)\n",
    "if os.path.exists(fname_raw_ec_run):\n",
    "    print('found raw eyes closed: \\t\\t' + os.path.basename(fname_raw_ec_run))\n",
    "    raw_ec = mne.io.read_raw_fif(fname_raw_ec_run, preload=True)\n",
    "if os.path.exists(fname_ica_ocular_rejection):\n",
    "    print('found ica eyeblink rejection:\\t' + os.path.basename(fname_ica_ocular_rejection))\n",
    "    with open(fname_ica_ocular_rejection,'rb') as f:\n",
    "        ica_rejection = pickle.load(f)\n",
    "if os.path.exists(fname_csp_alpha_rejection):\n",
    "    print('found csp alpha rejection: \\t' + os.path.basename(fname_csp_alpha_rejection))\n",
    "    with open(fname_csp_alpha_rejection, 'rb') as f:\n",
    "        csp_rejection = pickle.load(f)\n",
    "\n",
    "if os.path.exists(fname_calibration_envelopes):\n",
    "    print('found calibration envelopes: \\t' + os.path.basename(fname_calibration_envelopes))\n",
    "    with open(fname_calibration_envelopes, 'rb') as f: \n",
    "        calibration_envelopes = pickle.load(f)\n",
    "    locals().update(calibration_envelopes)\n",
    "if os.path.exists(fname_calibration_parameters):\n",
    "    print('found calibration parameters: \\t' + os.path.basename(fname_calibration_parameters))\n",
    "    with open(fname_calibration_parameters, 'rb') as f: \n",
    "        calibration_parameters = pickle.load(f)\n",
    "    locals().update(calibration_parameters)\n",
    "        \n",
    "if os.path.exists(fname_raw_nftraining_run):\n",
    "    print('found raw nftraining: \\t\\t' + os.path.basename(fname_raw_nftraining_run))\n",
    "    raw_nftraining = mne.io.read_raw_fif(fname_raw_nftraining_run)\n",
    "if os.path.exists(fname_rtanalyzed_nftraining_eeg):\n",
    "    print('found rt-analyzed eeg: \\t\\t' + os.path.basename(fname_rtanalyzed_nftraining_eeg))\n",
    "    rtanalyzed_nftraining_eeg = mne.io.read_raw_fif(fname_rtanalyzed_nftraining_eeg)\n",
    "if os.path.exists(fname_rtanalyzed_nftraining_emg):\n",
    "    print('found rt-analyzed emg: \\t\\t' + os.path.basename(fname_rtanalyzed_nftraining_emg))\n",
    "    rtanalyzed_nftraining_emg = mne.io.read_raw_fif(fname_rtanalyzed_nftraining_emg)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Fix/check the EEG Cap\n",
    "\n",
    "- fix the EEG Cap\n",
    "- check with openBCI GUI if the signals look OK, once they do:\n",
    "- run the `python raw_eo_data --stream`, followed by `/start`\n",
    "- the light on the usb stick should go <font color=\"red\">RED</font>\n",
    "- (re)-start the Cap or USB if it doesn't work, followed by commands above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Connect to the real-time Data Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pylsl.pylsl.StreamInfo at 0x7f0674946be0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pylsl.resolve_streams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this_is_eeg_']\n"
     ]
    }
   ],
   "source": [
    "# prints out which streams are currently available\n",
    "stream_ids = [ pylsl.stream_inlet(s).info().source_id() for s in pylsl.resolve_streams() ]\n",
    "print(stream_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- copy/paste the eeg lab name (left of the 2 outputs) into stream_id variable:\n",
    "- thake the one that says **eeg**, not aux!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this_is_eeg_\n"
     ]
    }
   ],
   "source": [
    "# 'subscribe' to a data stream; grab all essential(s), fs, names, etc\n",
    "# stream_id = 'openbci_eeg_id134'\n",
    "\n",
    "# eeg_stream_id = [stream_id for stream_id in stream_ids if re.match('.*_eeg_.*', stream_id)]\n",
    "eeg_stream_id = [stream_id for stream_id in stream_ids if re.match('.*eeg.*', stream_id)]\n",
    "if len(eeg_stream_id)>0:\n",
    "    eeg_stream_id=eeg_stream_id[0]\n",
    "print(eeg_stream_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name = Playback_eeg_3\n",
      "sampling_freq = 125\n",
      "channel_count = 16\n",
      "channel_format = 1\n"
     ]
    }
   ],
   "source": [
    "# try grabbing all the information from that stream:\n",
    "data_stream=pylsl.resolve_byprop(\"source_id\", eeg_stream_id, timeout=5.0)\n",
    "if data_stream:\n",
    "    data_inlet=pylsl.stream_inlet(data_stream[0], max_buflen=10)\n",
    "    stream_info = data_inlet.info()\n",
    "    stream_Fs = stream_info.nominal_srate()\n",
    "    stream_xml = stream_info.desc()\n",
    "    chans_xml = stream_xml.child(\"channels\")\n",
    "    chan_xml_list = []\n",
    "    ch = chans_xml.child(\"channel\")\n",
    "    while ch.name() == \"channel\":\n",
    "        chan_xml_list.append(ch)\n",
    "        ch = ch.next_sibling(\"channel\")\n",
    "    channel_names = [ch_xml.child_value(\"label\") for ch_xml in chan_xml_list]\n",
    "    data_inlet_dt = data_inlet.time_correction(timeout=5.0)\n",
    "    sampling_freq = data_stream[0].nominal_srate()\n",
    "    print('name = %s' % data_stream[0].name())\n",
    "    print('sampling_freq = %d' % sampling_freq)\n",
    "    print('channel_count = %d' % data_stream[0].channel_count())\n",
    "    print('channel_format = %d' % data_stream[0].channel_format())\n",
    "else:\n",
    "    raise Exception('No Data Stream Found - Is your EEG Cap running?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data acquisition for Calibrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Measure Some Data (Eye Open)\n",
    "- Inform that the following measurement is an eye open measurement.\n",
    "- Subjects are allowed to blink as normal\n",
    "- duration about 2 minutes\n",
    "- might have to re-run this in order to fix a channel from being busted\n",
    "- when ready press 'stop acquisition' and close window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put the incoming data in here:\n",
    "np_eo = dynarray.DynamicArray((None, len(channel_names))); times_of_samples = []\n",
    "\n",
    "# init/open the window in which we visualize data\n",
    "w=guis.AcquireData(sampling_freq, channel_names)\n",
    "\n",
    "# before we start, pull everything from the buffer (empty it)\n",
    "data_inlet.pull_chunk()\n",
    "while data_inlet.samples_available(): data_inlet.pull_chunk() \n",
    "    \n",
    "# then start acquiring data as long as button 'stop' not pressed:\n",
    "w.RUNLOOP=True\n",
    "while w.RUNLOOP:\n",
    "\n",
    "    time.sleep(0.001)\n",
    "    if not data_inlet.samples_available():\n",
    "        w.update(None)\n",
    "    else:\n",
    "        chunk_data, chunk_times = data_inlet.pull_chunk(timeout=0.0) # grab from LSL\n",
    "\n",
    "        \n",
    "        np_eo.extend(chunk_data) # add to our list\n",
    "        times_of_samples.append(chunk_times)\n",
    "        w.update(chunk_data) # update the GUI window       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we make the MNE data file from it\n",
    "raw_eo = mne.io.RawArray(np.transpose(np_eo)*1E-6,\n",
    "                        mne.create_info(channel_names, \n",
    "                                    sampling_freq, \n",
    "                                    detect_channel_types(channel_names), \n",
    "                                    'standard_1020'\n",
    "                        )\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Inspect the eo data  (run1)\n",
    "- scroll through the data see if the EEG signal is what you expect\n",
    "- mark bad channels (that cannot be rescued)\n",
    "- mark bad segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a simple display filter\n",
    "raw_eo_copy = raw_eo.copy()\n",
    "raw_eo_copy.notch_filter((25, 50)).filter(1, 40).plot(title='eyes open');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save this raw data to disk (filename taken care of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-11/ses-01/eeg/sub-11_ses-01_task-eo_run-01.fif\n",
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-11/ses-01/eeg/sub-11_ses-01_task-eo_run-01_acqtimes.pkl\n",
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-11/ses-01/eeg/sub-11_ses-01_task-eo_run-01_acqtimes.mat\n"
     ]
    }
   ],
   "source": [
    "raw_eo.set_annotations(raw_eo_copy.annotations)\n",
    "raw_eo.info = raw_eo_copy.info\n",
    "\n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'eo', run)\n",
    "fname_raw_eo_run = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "raw_eo.save(fname_raw_eo_run, overwrite=True)\n",
    "spio.savemat(re.sub('.fif$','_acqtimes.mat', fname_raw_eo_run), {'times_of_samples':np.array(times_of_samples)})\n",
    "with open(re.sub('.fif$','_acqtimes.pkl', fname_raw_eo_run), 'wb') as f: pickle.dump(times_of_samples, f)\n",
    "    \n",
    "print('saved: ' + fname_raw_eo_run)\n",
    "print('saved: ' + re.sub('.fif$','_acqtimes.pkl', fname_raw_eo_run))\n",
    "print('saved: ' + re.sub('.fif$','_acqtimes.mat', fname_raw_eo_run))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Measure Some Data (Eye Closed)\n",
    "- Inform your subject to keep the eye closed, then start the measurement\n",
    "- duration about 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put the incoming data in here:\n",
    "np_ec = dynarray.DynamicArray((None, len(channel_names))); times_of_samples = []\n",
    "\n",
    "\n",
    "# init/open the window in which we visualize data\n",
    "w=guis.AcquireData(sampling_freq, channel_names)\n",
    "\n",
    "# before we start, pull everything from the buffer (empty it)\n",
    "data_inlet.pull_chunk()\n",
    "while data_inlet.samples_available(): data_inlet.pull_chunk() \n",
    "    \n",
    "# then start acquiring data as long as button 'stop' not pressed:\n",
    "w.RUNLOOP=True\n",
    "while w.RUNLOOP:\n",
    "\n",
    "    time.sleep(0.001)\n",
    "    if not data_inlet.samples_available():\n",
    "        w.update(None)\n",
    "    else:\n",
    "        \n",
    "        chunk_data, chunk_times = data_inlet.pull_chunk(timeout=0.0) # grab from LSL\n",
    "\n",
    "        np_ec.extend(chunk_data) # add to our list\n",
    "        times_of_samples.append(chunk_times)\n",
    "        w.update(chunk_data) # update the GUI window        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting bad chanels in the data set to be the same as previous dataset\n",
      "[]\n",
      "Do not change bad channels further - re-do eo and ec if needed!\n"
     ]
    }
   ],
   "source": [
    "# and we make the MNE data file from it        \n",
    "raw_ec = mne.io.RawArray(np.transpose(np_ec)*1E-6,\n",
    "                        mne.create_info(channel_names, \n",
    "                                    sampling_freq, \n",
    "                                    detect_channel_types(channel_names), \n",
    "                                    'standard_1020'\n",
    "                               )\n",
    "                       )\n",
    "raw_ec.info['bads'] = raw_eo.info['bads']\n",
    "print('setting bad chanels in the data set to be the same as previous dataset')\n",
    "print(raw_ec.info['bads'])\n",
    "print('Do not change bad channels further - re-do eo and ec if needed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Inspect the ec data\n",
    "- scroll through the data see if the EEG signal is what you expect\n",
    "- mark bad channels (that cannot be rescued)\n",
    "- mark bad segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a simple display filter\n",
    "raw_ec_copy = raw_ec.copy()\n",
    "raw_ec_copy.filter(1, 35).plot(title='eyes closed');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check bad channels in eo, compare to ec\n",
    "if not raw_ec_copy.info['bads'] == raw_eo.info['bads']:\n",
    "    # raise Exception('Bad Channels are not the same between the two datasets - fix this first')\n",
    "    all_bads = list(set(raw_ec_copy.info['bads'] + raw_eo.info['bads']))\n",
    "    raw_eo.info['bads'] = all_bads\n",
    "    raw_ec.info['bads'] = all_bads\n",
    "    \n",
    "    # save raw_eo again\n",
    "    raw_eo.save(fname_raw_eo_run, overwrite=True)\n",
    "    print('saved AGAIN: ' + fname_raw_eo_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save this raw data to disk (filename taken care of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-11/ses-01/eeg/sub-11_ses-01_task-ec_run-01.fif\n",
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-11/ses-01/eeg/sub-11_ses-01_task-ec_run-01_acqtimes.pkl\n",
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-11/ses-01/eeg/sub-11_ses-01_task-ec_run-01_acqtimes.mat\n"
     ]
    }
   ],
   "source": [
    "raw_ec.set_annotations(raw_ec_copy.annotations)\n",
    "raw_ec.info = raw_ec_copy.info\n",
    "\n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'ec', run)\n",
    "fname_raw_ec_run = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "raw_ec.save(fname_raw_ec_run, overwrite=True)\n",
    "spio.savemat(re.sub('.fif$','_acqtimes.mat', fname_raw_ec_run), {'times_of_samples':np.array(times_of_samples)})\n",
    "with open(re.sub('.fif$','_acqtimes.pkl', fname_raw_ec_run), 'wb') as f: pickle.dump(times_of_samples, f)\n",
    "    \n",
    "print('saved: ' + fname_raw_ec_run)\n",
    "print('saved: ' + re.sub('.fif$','_acqtimes.pkl', fname_raw_ec_run))\n",
    "print('saved: ' + re.sub('.fif$','_acqtimes.mat', fname_raw_ec_run))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Preprocessing Calibations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Calculate the Eyeblink Spatial Rejection\n",
    "- this will automatically read in the EO data  (run1)\n",
    "- and run the ICA analysis\n",
    "- your job is to select the component most resembling ocular artifact\n",
    "- and the matrix will/should be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply filter: 3 to 45\n",
      "Dropped 250 outliers\n",
      "ICA/CSP time elapsed = 14.968744277954102s\n",
      "Table drawing time elapsed = 4.225008964538574s\n",
      "Created an ICA Spatial Filter\n",
      "<pynfb.signal_processing.filters.SpatialRejection object at 0x7fa7bd134ba8>\n"
     ]
    }
   ],
   "source": [
    "# apply the annotations and bad channel mask to get a matrix of 'good' data:\n",
    "clean_raw = no_bad_channels(\n",
    "    no_bad_samples(\n",
    "        raw_eo.filter(3, 40).notch_filter((25, 31.25, 50))\n",
    "    )\n",
    ");\n",
    "\n",
    "\n",
    "# run the ICA analysis -- the ICA analysis already applies a 3-40 Hz filter!!\n",
    "from pynfb.protocols.ssd.topomap_selector_ica import ICADialog\n",
    "\n",
    "ica_rejection, _, _, ica_unmixing_matrix, _, _ = ICADialog.get_rejection(\n",
    "    clean_raw.get_data().T, \n",
    "    clean_raw.ch_names,\n",
    "    clean_raw.info['sfreq'],\n",
    "    decomposition=None\n",
    ")\n",
    "\n",
    "\n",
    "bad_channel_mask = [ch not in raw_eo.info['bads'] for ch in raw_eo.ch_names]\n",
    "ica_rejection = ica_rejection.expand_by_mask(bad_channel_mask)\n",
    "\n",
    "print('Created an ICA Spatial Filter')\n",
    "print(ica_rejection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Save the eyeblink Rejection\n",
    "- close all open windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-02/ses-14/eeg/sub-02_ses-14_ica-ocular-rejection_run-01.pkl\n"
     ]
    }
   ],
   "source": [
    "# we save the ICA for ocular rejection:\n",
    "# save the ICA rejection \n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_ica-ocular-rejection_run-{:02d}.pkl'.format(sub, ses, run)\n",
    "fname_ica_ocular_rejection = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "with open(fname_ica_ocular_rejection, 'wb') as f: pickle.dump(ica_rejection, f)\n",
    "print('saved: ' + fname_ica_ocular_rejection)\n",
    "\n",
    "# save it also as a .txt matrix (for matlab)\n",
    "np.savetxt(re.sub('.pkl$','.txt', fname_ica_ocular_rejection), ica_rejection.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Check the eyeblink Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot before ICA\n",
    "no_bad_samples(raw_eo.copy().filter(1, None).notch_filter((25, 31.25, 50))).plot(title='before ICA');\n",
    "\n",
    "# plot after ICA\n",
    "raw_eo_ica_applied = mne.io.RawArray(ica_rejection.apply(raw_eo.get_data().T).T, \n",
    "        mne.create_info(raw_eo.ch_names, \n",
    "                        raw_eo.info['sfreq'],\n",
    "                        detect_channel_types(raw_eo.ch_names), \n",
    "                        'standard_1020')\n",
    "               )\n",
    "raw_eo_ica_applied.set_annotations(raw_eo.annotations)\n",
    "no_bad_samples(raw_eo_ica_applied.copy().filter(1, None).notch_filter((25, 31.25, 50))).plot(title='after ICA');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Handle Alpha Power Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we concatenate the 'eo' and the 'ec' data, and put it into a giant 'raw' (compare with and without)\n",
    "#if 'clean_raw_eo' in locals():\n",
    "#    del(clean_raw_eo)\n",
    "clean_raw_eo = no_bad_channels(\n",
    "    no_bad_samples(\n",
    "        raw_eo.copy().filter(3, 40).notch_filter((25, 31.25, 50))\n",
    "    )\n",
    ")\n",
    "\n",
    "clean_raw_ec = no_bad_channels(\n",
    "    no_bad_samples(\n",
    "        raw_ec.copy().filter(3, 40).notch_filter((25, 31.25, 50))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RawArray  |  None, n_channels x n_times : 16 x 31665 (253.3 sec), ~3.9 MB, data loaded>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_channel_mask = [ch not in raw_eo.info['bads'] for ch in raw_eo.ch_names]\n",
    "shrunk_ica_rejection = ica_rejection.shrink_by_mask(bad_channel_mask)\n",
    "\n",
    "eoec = mne.concatenate_raws([clean_raw_eo.copy(), clean_raw_ec])\n",
    "\n",
    "eoec_ica_applied = mne.io.RawArray(\n",
    "    shrunk_ica_rejection.apply(np.hstack((clean_raw_eo.get_data(), clean_raw_ec.get_data())).T).T,\n",
    "    mne.create_info(clean_raw_eo.ch_names, \n",
    "                        clean_raw_eo.info['sfreq'],\n",
    "                        detect_channel_types(clean_raw_eo.ch_names), \n",
    "                        'standard_1020')\n",
    ")\n",
    "\n",
    "eoec_ica_applied.set_annotations(eoec.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply filter: 3 to 45\n",
      "Dropped 240 outliers\n",
      "ICA/CSP time elapsed = 0.08969426155090332s\n",
      "Table drawing time elapsed = 4.206964731216431s\n"
     ]
    }
   ],
   "source": [
    "# label what is 'eo' and what is 'ec':\n",
    "labels_for_csp = np.hstack((np.zeros(clean_raw_eo.last_samp+1), np.ones(clean_raw_ec.last_samp+1)))\n",
    "\n",
    "    \n",
    "# bring up the GUI for CSP filtering of the data:\n",
    "csp_rejection, filter, topography, _, bandpass, to_all = ICADialog.get_rejection(\n",
    "    eoec_ica_applied.get_data().T,\n",
    "    eoec_ica_applied.ch_names, \n",
    "    eoec_ica_applied.info['sfreq'],\n",
    "    mode='csp', \n",
    "    _stimulus_split=False,\n",
    "    labels=labels_for_csp, # will convert to 0-1 vector for each sample in x\n",
    "    marks=None)\n",
    "\n",
    "bad_channel_mask = [ch not in raw_eo.info['bads'] for ch in raw_eo.ch_names]\n",
    "csp_rejection = csp_rejection.expand_by_mask(bad_channel_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Save the Alpha Power Rejection\n",
    "- close all open windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-02/ses-14/eeg/sub-02_ses-14_csp-alpha-rejection_run-01.pkl\n"
     ]
    }
   ],
   "source": [
    "# we save the ICA for ocular rejection:\n",
    "# save the ICA rejection \n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_csp-alpha-rejection_run-{:02d}.pkl'.format(sub, ses, run)\n",
    "fname_csp_alpha_rejection = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "with open(fname_csp_alpha_rejection, 'wb') as f: pickle.dump(csp_rejection, f)\n",
    "print('saved: ' + fname_csp_alpha_rejection)\n",
    "\n",
    "# save it also as .txt matrix (for matlab)\n",
    "np.savetxt(re.sub('.pkl$','.txt', fname_csp_alpha_rejection), csp_rejection.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Inspect Alpha Power Rejection\n",
    "- close all open windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RawArray  |  None, n_channels x n_times : 16 x 31665 (253.3 sec), ~3.9 MB, data loaded>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_channel_mask = [ch not in raw_eo.info['bads'] for ch in raw_eo.ch_names]\n",
    "shrunk_csp_rejection = csp_rejection.shrink_by_mask(bad_channel_mask)\n",
    "\n",
    "eoec_csp_and_ica_applied = mne.io.RawArray(\n",
    "    shrunk_csp_rejection.apply(eoec_ica_applied.get_data().T).T,\n",
    "    mne.create_info(eoec_ica_applied.ch_names, \n",
    "                        eoec_ica_applied.info['sfreq'],\n",
    "                        detect_channel_types(eoec_ica_applied.ch_names), \n",
    "                        'standard_1020',\n",
    "               )\n",
    ")\n",
    "\n",
    "eoec_csp_and_ica_applied.set_annotations(eoec_ica_applied.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eoec.plot(title='no spatial filters');\n",
    "\n",
    "eoec_ica_applied.plot(title='eyeblink rejection (ICA)');\n",
    "\n",
    "eoec_csp_and_ica_applied.plot(title='eyeblink + alpha rejection (ICA followed by CSP)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check power:\n",
    "fraw = eoec.plot_psd(); fraw.suptitle('no spatial filters');\n",
    "fica = eoec_ica_applied.plot_psd(); fica.suptitle('ica (eyeblinks) applied');\n",
    "fcspica = eoec_csp_and_ica_applied.plot_psd(); fcspica.suptitle('csp (alpha) and ica (eyeblinks) applied');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Talk to Stimulus Computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Create Simulus Handler, I\n",
    "- connect via WiFi\n",
    "    - ip address = 10.42.0.1\n",
    "    - host = stim-pc\n",
    "    - password = PASSWORD, OR 12345678\n",
    "- ideally this should already be up and running, so you can skip over these more fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callpyff import bcinetwork, bcixml\n",
    "bcinet = bcinetwork.BciNetwork('10.42.0.1', bcinetwork.FC_PORT, bcinetwork.GUI_PORT, 'bcixml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bcinet.getAvailableFeedbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcinet.send_init('BrainWaveTraining_II')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the stimulus about the monitor\n",
    "bcinet.send_signal(bcixml.BciSignal({'EX_TESTNFNOISE': False},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'EX_PR_SLEEPTIME': 0.005},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'MONITOR_PIXWIDTH': 1366},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'MONITOR_PIXHEIGHT': 768},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'MONITOR_FULLSCR': True},None, bcixml.INTERACTION_SIGNAL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Start Stimulus Marker Receiver\n",
    " - this will listen on UDP port 6500 for any incoming markers\n",
    " - This is to convert signals from the Presentation into annotations\n",
    " - grab markers in the NF loop with: `while not marker_queue.empty():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell Stimulus computer to send markers to our interface wlp58s0 with ip=192.168.1.43 and port=6500\n"
     ]
    }
   ],
   "source": [
    "import netifaces\n",
    "default_interface=netifaces.gateways()['default'][netifaces.AF_INET][1]\n",
    "ip_address=netifaces.ifaddresses(default_interface)[netifaces.AF_INET][0]['addr']\n",
    "port = 6500\n",
    "print('Tell Stimulus computer to send markers to our interface {} with ip={} and port={}'.format(default_interface, ip_address, port))\n",
    "\n",
    "bcinet.send_signal(bcixml.BciSignal({'EVENT_destip': ip_address},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'EVENT_destport': port},None, bcixml.INTERACTION_SIGNAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening on :6500\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def my_marker_server(marker_queue, PORT):\n",
    "    # simple markers server.\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    sock.bind(('', PORT))\n",
    "    print(\"Listening on {}:{}\".format('', PORT))\n",
    "    while True:\n",
    "        data = sock.recv(1024)\n",
    "        if data == b'stop_server':\n",
    "            break\n",
    "        marker_queue.put(data)\n",
    "\n",
    "    print('finished')\n",
    "    sock.close()\n",
    "    \n",
    "def send_stop_it(port):\n",
    "    sock=socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    sock.sendto(b'stop_server', ('localhost', port))\n",
    "    sock.close()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "if not 'marker_queue' in locals():\n",
    "    marker_queue = Queue()\n",
    "if 'my_udp_server' in locals():\n",
    "    # we stop the marker server\n",
    "    send_stop_it(port)\n",
    "    try:\n",
    "        # and we try to join it\n",
    "        my_udp_server.join()\n",
    "    except:\n",
    "        del(my_udp_server)\n",
    "        \n",
    "my_udp_server = Process(target=my_marker_server, args=(marker_queue, port))\n",
    "my_udp_server.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Define RT Signal analysis flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.1 RT Signal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynfb.signal_processing.filters import (FilterSequence, \n",
    "                                             CFIRBandEnvelopeDetector, \n",
    "                                             ExponentialSmoother,\n",
    "                                             SpatialFilter,\n",
    "                                             ButterFilter,\n",
    "                                             ButterBandEnvelopeDetector,\n",
    "                                             ScalarButterFilter,\n",
    "                                             MASmoother,\n",
    "                                             FFTBandEnvelopeDetector,\n",
    "                                             NotchFilter\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Filter Sequence for NF\n",
    "\n",
    "# Which channel do we select for the EEG - we do C3.\n",
    "rt_eeg_channels = ['C4']\n",
    "rt_eeg_channels_mask = np.where([ch in rt_eeg_channels for ch in channel_names], 1, 0)/len(rt_eeg_channels)\n",
    "eeg_channel_select = SpatialFilter(rt_eeg_channels_mask)\n",
    "highpass_filter = ScalarButterFilter([3, None], sampling_freq)\n",
    "butter_filter = ScalarButterFilter([12, 15], sampling_freq)\n",
    "\n",
    "# eeg_notch_50 = NotchFilter(50, sampling_freq, len(channel_names))\n",
    "# eeg_notch_25 = NotchFilter(25, sampling_freq, len(channel_names))\n",
    "\n",
    "preprocess_filters_eeg = FilterSequence([\n",
    "    ica_rejection,\n",
    "    csp_rejection,\n",
    "    eeg_channel_select,\n",
    "    highpass_filter\n",
    "    \n",
    "])\n",
    "envelope_filter_eeg = CFIRBandEnvelopeDetector([12, 15], sampling_freq, MASmoother(round(sampling_freq/5)))\n",
    "butter_visualization_eeg = ButterFilter([12, 15], sampling_freq, 1)\n",
    "\n",
    "\n",
    "# Processing of the EMG - this is basically our second channel...\n",
    "rt_emg_channels = ['T3','T4','Fp1','Fp2']\n",
    "# rt_emg_channels = ['T7','T8','TP9','TP10']\n",
    "rt_emg_channels_mask = np.where([ch in rt_emg_channels for ch in channel_names], 1, 0)/len(rt_emg_channels)\n",
    "emg_channel_select = SpatialFilter(rt_emg_channels_mask)\n",
    "\n",
    "preprocess_filters_emg = FilterSequence([\n",
    "    emg_channel_select,\n",
    "])\n",
    "\n",
    "envelope_filter_emg = ButterBandEnvelopeDetector([60, 62], sampling_freq, MASmoother(round(sampling_freq/5)))\n",
    "butter_visualization_emg = ButterFilter([55, 60], sampling_freq, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Appy Rt Filters on Calibration Data\n",
    "This will:\n",
    "- apply the RT filters to our existing data\n",
    "- calibrate the signaltracking objects (that send stuff to the Stimulus) properly\n",
    "- set all the scalings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eoec_rawraw = mne.concatenate_raws([raw_eo.copy(), raw_ec])\n",
    "# eoec_rawraw.plot();\n",
    "\n",
    "preprocessed_eeg = preprocess_filters_eeg.apply(eoec_rawraw.get_data().T)*1E6\n",
    "preprocessed_emg = preprocess_filters_emg.apply(eoec_rawraw.get_data().T)*1E6\n",
    "\n",
    "eeg_vec = envelope_filter_eeg.apply(preprocessed_eeg)\n",
    "emg_vec = envelope_filter_emg.apply(preprocessed_emg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-02/ses-14/eeg/sub-02_ses-14_calibration_envelopes_run-01.pkl\n",
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-02/ses-14/eeg/sub-02_ses-14_calibration_envelopes_run-01.mat\n"
     ]
    }
   ],
   "source": [
    "# save the eeg_vec and emg_vec also as .pkl and .mat file\n",
    "calibration_envelopes = {'eeg_vec':eeg_vec, 'emg_vec':emg_vec, 'preprocessed_eeg': preprocessed_eeg}\n",
    "\n",
    "# save it\n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_calibration_envelopes_run-{:02d}.pkl'.format(sub, ses, run)\n",
    "fname_calibration_envelopes = os.path.join(this_save_dir, this_raw_fname)\n",
    "with open(fname_calibration_envelopes, 'wb') as f: pickle.dump(calibration_envelopes, f)\n",
    "print('saved: ' + fname_calibration_envelopes)\n",
    "spio.savemat(re.sub('.pkl$','.mat', fname_calibration_envelopes), calibration_envelopes)\n",
    "print('saved: ' + re.sub('.pkl$','.mat', fname_calibration_envelopes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Use calibration data to set thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eeg: mean 7.679, median 4.847, mode 4.312: \n",
      "emg: mean 2.443, median 0.702, mode 0.000: \n",
      "4.26 minutes of data\n",
      "mode_thr_eeg = 1.835\n",
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-02/ses-14/eeg/sub-02_ses-14_calibration_parameters_run-01.pkl\n",
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-02/ses-14/eeg/sub-02_ses-14_calibration_parameters_run-01.mat\n"
     ]
    }
   ],
   "source": [
    "butter_filter = ScalarButterFilter([12, 15], sampling_freq)\n",
    "\n",
    "mode_eeg = find_mode(butter_filter.apply(preprocessed_eeg), make_plot=True)\n",
    "median_eeg = np.median(eeg_vec)\n",
    "mean_eeg = np.mean(eeg_vec)\n",
    "print('eeg: mean {:2.3f}, median {:2.3f}, mode {:2.3f}: '.format(mean_eeg, median_eeg, mode_eeg))\n",
    "mode_fig = plt.gcf()\n",
    "\n",
    "mode_emg = None # we won't bother detecting mode of EMG with poor quality EMG\n",
    "median_emg = np.median(emg_vec)\n",
    "mean_emg = np.mean(emg_vec)\n",
    "print('emg: mean {:2.3f}, median {:2.3f}, mode {:2.3f}: '.format(mean_emg, median_emg, 0))\n",
    "\n",
    "print('{:3.2f} minutes of data'.format(len(preprocessed_eeg)/sampling_freq/60))\n",
    "\n",
    "# determine the threshold for 90 bursts!\n",
    "mode_thr_eeg, _, _ = determine_optimal_threshold(butter_filter.apply(preprocessed_eeg), sampling_freq, 0.2, 90, make_plot=True)\n",
    "print('mode_thr_eeg = {:2.3f}'.format(mode_thr_eeg))\n",
    "mode_thr_fig = plt.gcf()\n",
    "\n",
    "median_thr_emg = 10.\n",
    "\n",
    "EEG_THR = mode_thr_eeg * mode_eeg\n",
    "EEG_DUR = 0.20\n",
    "EEG_STSCALING = 10 * mode_eeg\n",
    "\n",
    "EMG_THR = median_thr_emg * median_emg\n",
    "EMG_DUR = 0.25\n",
    "EMG_STSCALING = 2 * EMG_THR\n",
    "\n",
    "\n",
    "\n",
    "calibration_parameters = {'mode_eeg':mode_eeg,\n",
    "                          'mode_emg':0.,\n",
    "                          'mode_thr_eeg':mode_thr_eeg,\n",
    "                          'median_thr_emg':median_thr_emg,\n",
    "                          'EEG_THR':EEG_THR, \n",
    "                          'EEG_DUR':EEG_DUR, \n",
    "                          'EEG_STSCALING': EEG_STSCALING, \n",
    "                          'EMG_THR':EMG_THR,\n",
    "                          'EMG_DUR':EMG_DUR,\n",
    "                          'EMG_STSCALING':EMG_STSCALING\n",
    "                         }\n",
    "\n",
    "# save those too\n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_calibration_parameters_run-{:02d}.pkl'.format(sub, ses, run)\n",
    "fname_calibration_parameters = os.path.join(this_save_dir, this_raw_fname)\n",
    "with open(fname_calibration_parameters, 'wb') as f: pickle.dump(calibration_parameters, f)\n",
    "print('saved: ' + fname_calibration_parameters)\n",
    "spio.savemat(re.sub('.pkl$','.mat', fname_calibration_parameters), calibration_parameters)\n",
    "print('saved: ' + re.sub('.pkl$','.mat', fname_calibration_parameters))\n",
    "\n",
    "mode_fig.savefig(os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_mode_run-{:02d}.png'.format(sub, ses, run)))\n",
    "mode_thr_fig.savefig(os.path.join(this_save_dir, 'sub-{:02d}_ses-{:02d}_threshold_run-{:02d}.png'.format(sub, ses, run)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Create Stimulus Handlers, II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr: 7.91, dur: 0.20\n",
      "bcinet is passed on\n",
      "thr: 7.02, dur: 0.25\n",
      "bcinet is passed on\n"
     ]
    }
   ],
   "source": [
    "# these objects can send variables over to the stimulus\n",
    "# parameters to convert the filtered EEG signal to the stimulus\n",
    "# the following parameter should come out of the EEG data, as our initial threshold to use:\n",
    "# global_std_band=5\n",
    "\n",
    "from nftools.nftools import signaltracking\n",
    "track_for_eeg_stimuli = signaltracking.sending_to_nfstim(\n",
    "    sampling_freq,\n",
    "    thr=EEG_THR, \n",
    "    dur=EEG_DUR, \n",
    "    feedback_type='eeg', \n",
    "    max4audio=1.2, \n",
    "    bcinet=bcinet, \n",
    "    st_scaling=EEG_STSCALING,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# parameters to convert the filtered EMG signal to the stimulus\n",
    "track_for_emg_stimuli = signaltracking.sending_to_nfstim(\n",
    "    sampling_freq,\n",
    "    thr=EMG_THR, \n",
    "    dur=EMG_DUR, \n",
    "    feedback_type='emg', \n",
    "    bcinet=bcinet, \n",
    "    st_scaling=EMG_STSCALING,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 The Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Start the NF Stimulation\n",
    " - you still have to press <ENTER> to actually start the stimulus, but Not Yet!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcinet.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 make the windows for plotting\n",
    "- move them somewhere convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our UI 'Experience' -- it can consist of 3 separate, movable windows (for now)\n",
    "# same window as before + 2 other windows - 1 for interaction with stim/thresholds; 1 for looking\n",
    "# at the analysis itself.\n",
    "\n",
    "w_acquire = guis.AcquireData(sampling_freq, channel_names)\n",
    "w_interaction = guis.NFChangeThresholds(track_for_eeg_stimuli, track_for_emg_stimuli)\n",
    "w_eeganalysis = guis.AnalyzeData(sampling_freq, ['EEG','env','thr','vmarker','amarker'],track_for_eeg_stimuli)\n",
    "w_emganalysis = guis.AnalyzeData(sampling_freq, ['EMG','env','thr','vmarker','amarker'],track_for_emg_stimuli)\n",
    "\n",
    "from PyQt5 import QtWidgets, QtGui\n",
    "class AllInOneWindow(QtWidgets.QWidget):\n",
    "    def __init__(self):\n",
    "        super(AllInOneWindow, self).__init__()\n",
    "        layout = QtWidgets.QGridLayout()\n",
    "        layout.addWidget(w_acquire, 1, 1, 1, 1)\n",
    "        layout.addWidget(w_interaction, 2, 1, 1, 1)\n",
    "        layout.addWidget(w_eeganalysis, 1, 2, 1, 1)\n",
    "        layout.addWidget(w_emganalysis, 2, 2, 1, 1)\n",
    "        \n",
    "        # remove the checkboxes of all except the EEG analysis window:\n",
    "        w_acquire.datawidget.plot_check_box.setChecked(False)\n",
    "        w_emganalysis.plot_check_box.setChecked(False)\n",
    "        \n",
    "        self.setLayout(layout)\n",
    "        self.show()\n",
    "\n",
    "\n",
    "# can we combine everything into 1 Big Gui -- YES\n",
    "big_window = AllInOneWindow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Staring the NF Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n",
      "it should never happen that a marker is sent when the length is smaller than needed: -1.666666666666667\n"
     ]
    }
   ],
   "source": [
    "# clear EEG Data buffer\n",
    "data_inlet.pull_chunk()\n",
    "while data_inlet.samples_available(): data_inlet.pull_chunk() \n",
    "\n",
    "# containers for data collection: \n",
    "\n",
    "data_nf = dynarray.DynamicArray((None, len(channel_names))); times_of_samples = []\n",
    "data_analysis_eeg = dynarray.DynamicArray((None, 5))\n",
    "data_analysis_emg = dynarray.DynamicArray((None, 5))\n",
    "\n",
    "# collect markers from the Stimulation:\n",
    "stim_Annotations = mne.Annotations(0, 0, 'Start NF Loop')\n",
    "\n",
    "# collect markers from the interaction GUI:\n",
    "w_interaction.GUI_Annotations.append(time.time()-w_interaction.begin_time, 0, 'startloop')\n",
    "\n",
    "# start the loop\n",
    "w_acquire.RUNLOOP=True\n",
    "acquisition_start = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "t_begin=time.time()\n",
    "while w_acquire.RUNLOOP:\n",
    "    \n",
    "    # wait 1 msec\n",
    "    time.sleep(0.001)\n",
    "    \n",
    "    \n",
    "    # check if the presentation gave any markers, and collect them\n",
    "    while not marker_queue.empty():\n",
    "        stim_Annotations.append(time.time()-t_begin, 0, marker_queue.get().decode())\n",
    "    \n",
    "    # handle the incoming data (if any):\n",
    "    if not data_inlet.samples_available(): \n",
    "        w_acquire.update(None)\n",
    "    else:\n",
    "        chunk_data, chunk_times = data_inlet.pull_chunk() # grab from LSL\n",
    "\n",
    "        # update signal window so we can see raw signals\n",
    "        w_acquire.update(chunk_data) \n",
    "        \n",
    "        # store the raw data (and times)\n",
    "        times_of_samples.append(chunk_times)\n",
    "        data_nf.extend(chunk_data)\n",
    "\n",
    "        \n",
    "        #\n",
    "        # EEG Signal processing\n",
    "        #\n",
    "        \n",
    "        # apply spatial and temporal filters to the raw signal; for EEG and EMG:\n",
    "        preprocessed_eeg = preprocess_filters_eeg.apply(chunk_data)        \n",
    "        envelope_eeg = envelope_filter_eeg.apply(preprocessed_eeg)\n",
    "\n",
    "\n",
    "        # check if it's above threshold or not; send markers to stimulus computer\n",
    "        visual_markers_eeg, audio_markers_eeg = track_for_eeg_stimuli.check_above_threshold(envelope_eeg)  \n",
    "        # send the signal  to stimulus computer, too\n",
    "        track_for_eeg_stimuli.send_data_signal(envelope_eeg) \n",
    "        \n",
    "\n",
    "        # visualize the processing steps\n",
    "        # analysis_names_eeg = ('EEG', 'env', 'thr', 'vmarker', 'amarker')\n",
    "        analysis_data_eeg = np.vstack((\n",
    "            np.abs(butter_visualization_eeg.apply(preprocessed_eeg[:, None])[:, 0]),\n",
    "            envelope_eeg,\n",
    "            track_for_eeg_stimuli.thr * np.ones(preprocessed_eeg.shape),\n",
    "            visual_markers_eeg,\n",
    "            audio_markers_eeg,\n",
    "        )).T\n",
    "        \n",
    "        data_analysis_eeg.extend(analysis_data_eeg)  # store it for later conversion\n",
    "        w_eeganalysis.update(analysis_data_eeg) # update analysis window\n",
    "\n",
    "        \n",
    "        #\n",
    "        # EMG Signal Processing\n",
    "        #\n",
    "        \n",
    "        # do the same for the EMG NF, too:\n",
    "        # apply spatial and temporal filters to the raw signal; for EEG and EMG:\n",
    "        preprocessed_emg = preprocess_filters_emg.apply(chunk_data)\n",
    "        envelope_emg = envelope_filter_emg.apply(preprocessed_emg)\n",
    "        \n",
    "        # check if it's above threshold or not; send markers to stimulus computer\n",
    "        visual_markers_emg, audio_markers_emg = track_for_emg_stimuli.check_above_threshold(envelope_emg) \n",
    "        # send the signal  to stimulus computer, too\n",
    "        track_for_emg_stimuli.send_data_signal(envelope_emg) \n",
    "\n",
    "        # visualize the processing steps\n",
    "        # analysis_names_emg = ('EMG', 'env', 'thr', 'vmarker', 'amarker')\n",
    "        analysis_data_emg = np.vstack((\n",
    "            np.abs(butter_visualization_emg.apply(preprocessed_emg[:, None])[:, 0]),\n",
    "            envelope_emg,\n",
    "            track_for_emg_stimuli.thr * np.ones(preprocessed_emg.shape),\n",
    "            visual_markers_emg,\n",
    "            audio_markers_emg,\n",
    "        )).T\n",
    "        \n",
    "        data_analysis_emg.extend(analysis_data_emg)  # store it for later conversion\n",
    "        w_emganalysis.update(analysis_data_emg) # update analysis window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Now the participant will do the training\n",
    "- start it up on the NF Laptop now\n",
    "- wait until the training is done\n",
    "- then stop the loop as normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 save all the gathered data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle annotations, since w_interaction starts before data acquisition\n",
    "annots = w_interaction.GUI_Annotations.copy()\n",
    "tdelta = datetime.strptime(acquisition_start,'%Y-%m-%d %H:%M:%S.%f').timestamp() - annots.orig_time\n",
    "annots.onset -= tdelta\n",
    "annots.orig_time=None\n",
    "\n",
    "starting_annots = annots[list(map(lambda d: re.match('start.*', d) is not None, annots.description))]\n",
    "starting_annots.onset = 0.1*np.ones(len(starting_annots))\n",
    "starting_annots.orig_time = None\n",
    "\n",
    "# to extract\n",
    "to_process = {'irest':'11', 'brest':'12','erest':'13', \n",
    " 'itrain':'31', 'btrain':'32','etrain':'33', \n",
    " 'itransfer':'41', 'btransfer':'42','etransfer':'43',\n",
    " 'hit':'170', 'p':'171', 'emg':'230'}\n",
    "\n",
    "# bookkeeping on the annotations! - make things nice, near, easy to understand...\n",
    "new_annots = mne.Annotations(0, 0, ''); new_annots.delete(0)\n",
    "for key, value in to_process.items():\n",
    "    tmp_annots = stim_Annotations[[i for i, d in enumerate(stim_Annotations.description) if re.fullmatch(d, value)]]\n",
    "    \n",
    "    for item in tmp_annots:\n",
    "        if item['description'] in ['11', '31', '41']:\n",
    "            # its an instruction\n",
    "            new_annots.append(item['onset'], 0, key)\n",
    "        if item['description'] in ['12', '32', '42']:\n",
    "            # it's the onset of a rest/training/transfer interval - where is the end?\n",
    "            to_find = '{}3'.format(item['description'][0])\n",
    "            \n",
    "            # find all endings -- item['onset'] is the beginning\n",
    "            all_endings = np.array([item['onset'] for i, item in enumerate(stim_Annotations) if item['description'] == to_find])\n",
    "            # find first ending that is positive rel to this beginning\n",
    "            this_duration = all_endings[next((i for i, j in enumerate(all_endings > item['onset']) if j), None)] - item['onset']\n",
    "            # inserrt new annotation.. remove 'b' from the to_process.\n",
    "            new_annots.append(item['onset'], this_duration, key[1:])\n",
    "        if item['description'] in ['170','171']:\n",
    "            new_annots.append(item['onset'], 0, key)\n",
    "        if item['description'] in ['230']:\n",
    "            new_annots.append(item['onset']-0.5, 1.0, 'BAD_' + key)\n",
    "            \n",
    "all_annotations = starting_annots + annots + new_annots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-11/ses-01/eeg/sub-11_ses-01_task-raw-nftraining_run-01.fif\n"
     ]
    }
   ],
   "source": [
    "# and we make the MNE data file from it        \n",
    "raw_nftraining = mne.io.RawArray(np.transpose(data_nf)*1E-6,\n",
    "                        mne.create_info(channel_names, \n",
    "                                    sampling_freq, \n",
    "                                    detect_channel_types(channel_names), \n",
    "                                    'standard_1020'\n",
    "                           )\n",
    "                     )\n",
    "# saving the raw data\n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'raw-nftraining', run)\n",
    "fname_raw_nftraining_run = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "raw_nftraining.set_annotations(all_annotations)\n",
    "raw_nftraining.save(fname_raw_nftraining_run, overwrite=True)\n",
    "print('saved: ' + fname_raw_nftraining_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-11/ses-01/eeg/sub-11_ses-01_task-rtanalyzed-nftraining-eeg_run-01.fif\n"
     ]
    }
   ],
   "source": [
    "# and we make the MNE data file from it\n",
    "analysis_names_eeg = ('EEG', 'env', 'thr', 'vmarker', 'amarker')\n",
    "rtanalyzed_nftraining_eeg = mne.io.RawArray(np.transpose(np.array(data_analysis_eeg) * [1E-6, 1E-6, 1E-6, 1, 1]),\n",
    "                        mne.create_info(analysis_names_eeg, \n",
    "                                    sampling_freq, \n",
    "                                    ['eeg','eeg','eeg','stim','stim'], \n",
    "                                    None)\n",
    "                       )\n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'rtanalyzed-nftraining-eeg', run)\n",
    "fname_rtanalyzed_nftraining_eeg = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "rtanalyzed_nftraining_eeg.set_annotations(all_annotations)\n",
    "rtanalyzed_nftraining_eeg.save(fname_rtanalyzed_nftraining_eeg, overwrite=True)\n",
    "print('saved: ' + fname_rtanalyzed_nftraining_eeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-11/ses-01/eeg/sub-11_ses-01_task-rtanalyzed-nftraining-emg_run-01.fif\n"
     ]
    }
   ],
   "source": [
    "# and we make the MNE data file from it\n",
    "analysis_names_emg = ('EMG', 'env', 'thr', 'vmarker', 'amarker')\n",
    "rtanalyzed_nftraining_emg = mne.io.RawArray(np.transpose(np.array(data_analysis_emg) * [1E-6, 1E-6, 1E-6, 1, 1]),\n",
    "                        mne.create_info(analysis_names_emg, \n",
    "                                    sampling_freq, \n",
    "                                    ['emg','emg','emg','stim','stim'], \n",
    "                                    None)\n",
    "                       )\n",
    "this_raw_fname = 'sub-{:02d}_ses-{:02d}_task-{}_run-{:02d}.fif'.format(sub, ses, 'rtanalyzed-nftraining-emg', run)\n",
    "fname_rtanalyzed_nftraining_emg = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "\n",
    "rtanalyzed_nftraining_emg.set_annotations(all_annotations)\n",
    "rtanalyzed_nftraining_emg.save(fname_rtanalyzed_nftraining_emg, overwrite=True)\n",
    "print('saved: ' + fname_rtanalyzed_nftraining_emg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtanalyzed_nftraining_eeg.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find any of the events you specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-59e3b52e9650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# visualize delay between a reported 'hit' (stimulus computer) and a signalled 'hit' (analysis computer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_evs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents_from_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrtanalyzed_nftraining_eeg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'hit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mepoched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEpochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrtanalyzed_nftraining_eeg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_evs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepoched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vmarker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconv_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sfreq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sfreq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</home/johan/.conda/envs/rt/lib/python3.7/site-packages/mne/externals/decorator.py:decorator-gen-59>\u001b[0m in \u001b[0;36mevents_from_annotations\u001b[0;34m(raw, event_id, regexp, use_rounding, chunk_duration, verbose)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rt/lib/python3.7/site-packages/mne/utils/_logging.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0muse_log_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     return FunctionMaker.create(\n\u001b[1;32m     92\u001b[0m         \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'return decfunc(%(signature)s)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rt/lib/python3.7/site-packages/mne/annotations.py\u001b[0m in \u001b[0;36mevents_from_annotations\u001b[0;34m(raw, event_id, regexp, use_rounding, chunk_duration, verbose)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     event_sel, event_id_ = _select_annotations_based_on_description(\n\u001b[0;32m--> 913\u001b[0;31m         annotations.description, event_id=event_id, regexp=regexp)\n\u001b[0m\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunk_duration\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rt/lib/python3.7/site-packages/mne/annotations.py\u001b[0m in \u001b[0;36m_select_annotations_based_on_description\u001b[0;34m(descriptions, event_id, regexp)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_sel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Could not find any of the events you specified.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mevent_sel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_id_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find any of the events you specified."
     ]
    }
   ],
   "source": [
    "# visualize delay between a reported 'hit' (stimulus computer) and a signalled 'hit' (analysis computer)\n",
    "new_evs, event_id = mne.events_from_annotations(rtanalyzed_nftraining_eeg, event_id={'hit':2})\n",
    "epoched=mne.Epochs(rtanalyzed_nftraining_eeg, new_evs, 2, tmin=-0.8, tmax=0.1);\n",
    "epoched.drop(np.squeeze(np.sum(epoched.get_data(picks='vmarker'),axis=2)!=1))\n",
    "conv_func = scipy.signal.gaussian(epoched.info['sfreq']/10, std=epoched.info['sfreq']/20)\n",
    "m = np.squeeze(epoched.get_data(picks='vmarker'))\n",
    "new_m = np.apply_along_axis(lambda m: np.convolve(m, conv_func, mode='same'), axis=1, arr=m)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(epoched.times, np.mean(new_m, axis=0))\n",
    "# plt.plot(epoched.times, new_m.T)\n",
    "plt.xlabel('time (s)')\n",
    "plt.title('delay between stimulus & analysis computers for NF \\'hits\\'');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
