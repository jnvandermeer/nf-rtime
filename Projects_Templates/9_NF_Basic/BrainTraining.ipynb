{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Training\n",
    "This is the first notebook where everything regarding NF Training will come together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation I: modules and where to put files\n",
    "- double check all these parameters\n",
    "- change sub and sess according to the current participant and session (!)\n",
    "- double check the destination directory to be sure\n",
    "- don't use the Template Notebook (if you do so...), but copy/paste a notebook into a subject/session specific directory!\n",
    "- do not do this work just before a measurement. Have it prepared (and preferable tested!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import (most of the) modules that we need\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from multiprocessing import Process\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pylsl\n",
    "from nftools import guis\n",
    "import mne\n",
    "import dynarray\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the qt event loop, disable warnings (they flood the screen)\n",
    "%matplotlib qt  \n",
    "# %gui qt\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sub, session and run numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these according to which sub-sess we have:\n",
    "sub = 1\n",
    "sess = 1\n",
    "run = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  /home/johan/nf/rawdata/BrainTraining/bids/sub-01/sess-01/eeg  already exists\n"
     ]
    }
   ],
   "source": [
    "# define where we put our stuff - prepare for BIDS Format Style\n",
    "save_dir = '/home/johan/nf/rawdata/BrainTraining/bids'\n",
    "this_save_dir = os.path.join(save_dir, 'sub-{:02d}'.format(sub), 'sess-{:02d}'.format(sess), 'eeg')\n",
    "\n",
    "# we should also ... MAKE this savedir! If it exists, we don't do anything.\n",
    "if not os.path.exists(this_save_dir):\n",
    "    os.makedirs(this_save_dir)\n",
    "    print(\"Directory \" , this_save_dir ,  \" Created \")\n",
    "else:    \n",
    "    print(\"Directory \" , this_save_dir ,  \" already exists\")   \n",
    "    \n",
    "    \n",
    "# expected files to be read/written:\n",
    "# \n",
    "# fname_raw_eo_run1\n",
    "# fname_raw_ec_run2\n",
    "# fname_ica_ocular_rejection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparion, II: Fix/check the EEG Cap\n",
    "\n",
    "- fix the EEG Cap\n",
    "- check with openBCI GUI if the signals look OK, once they do:\n",
    "- run the `python raw_eo_data --stream`, followed by `/start`\n",
    "- the light on the usb stick should go <font color=\"red\">RED</font>\n",
    "- (re)-start the Cap or USB if it doesn't work, followed by commands above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the real-time Data Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['openbci_aux_id92', 'openbci_eeg_id92']\n"
     ]
    }
   ],
   "source": [
    "# prints out which streams are currently available\n",
    "stream_ids = [ pylsl.stream_inlet(s).info().source_id() for s in pylsl.resolve_streams() ]\n",
    "print(stream_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- copy/paste the eeg lab name (left of the 2 outputs) into stream_id variable:\n",
    "- thake the one that says **eeg**, not aux!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openbci_eeg_id92\n"
     ]
    }
   ],
   "source": [
    "# 'subscribe' to a data stream; grab all essential(s), fs, names, etc\n",
    "# stream_id = 'openbci_eeg_id134'\n",
    "\n",
    "eeg_stream_id = [stream_id for stream_id in stream_ids if re.match('.*_eeg_.*', stream_id)][0]\n",
    "print(eeg_stream_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name = openbci_eeg\n",
      "sampling_freq = 125\n",
      "channel_count = 16\n",
      "channel_format = 1\n"
     ]
    }
   ],
   "source": [
    "# try grabbing all the information from that stream:\n",
    "data_stream=pylsl.resolve_byprop(\"source_id\", eeg_stream_id, timeout=5.0)\n",
    "if data_stream:\n",
    "    data_inlet=pylsl.stream_inlet(data_stream[0], max_buflen=10)\n",
    "    stream_info = data_inlet.info()\n",
    "    stream_Fs = stream_info.nominal_srate()\n",
    "    stream_xml = stream_info.desc()\n",
    "    chans_xml = stream_xml.child(\"channels\")\n",
    "    chan_xml_list = []\n",
    "    ch = chans_xml.child(\"channel\")\n",
    "    while ch.name() == \"channel\":\n",
    "        chan_xml_list.append(ch)\n",
    "        ch = ch.next_sibling(\"channel\")\n",
    "    channel_names = [ch_xml.child_value(\"label\") for ch_xml in chan_xml_list]\n",
    "    data_inlet_dt = data_inlet.time_correction(timeout=5.0)\n",
    "    sampling_freq = data_stream[0].nominal_srate()\n",
    "    print('name = %s' % data_stream[0].name())\n",
    "    print('sampling_freq = %d' % sampling_freq)\n",
    "    print('channel_count = %d' % data_stream[0].channel_count())\n",
    "    print('channel_format = %d' % data_stream[0].channel_format())\n",
    "else:\n",
    "    raise Exception('No Data Stream Found - Is your EEG Cap running?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Some Data (Eye Open)\n",
    "- Inform that the following measurement is an eye open measurement.\n",
    "- Subjects are allowed to blink as normal\n",
    "- duration about 2 minutes\n",
    "- might have to re-run this in order to fix a channel from being busted\n",
    "- when ready press 'stop acquisition' and close window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put the incoming data in here:\n",
    "np_eo = dynarray.DynamicArray((None, len(channel_names)))\n",
    "\n",
    "# init/open the window in which we visualize data\n",
    "w=guis.AcquireData(sampling_freq, channel_names)\n",
    "\n",
    "# before we start, pull everything from the buffer (empty it)\n",
    "data_inlet.pull_chunk()\n",
    "while data_inlet.samples_available(): data_inlet.pull_chunk() \n",
    "    \n",
    "# then start acquiring data as long as button 'stop' not pressed:\n",
    "w.RUNLOOP=True\n",
    "while w.RUNLOOP:\n",
    "\n",
    "    if not data_inlet.samples_available():\n",
    "        w.update(None)\n",
    "    else:\n",
    "        \n",
    "        chunk_data, chunk_times = data_inlet.pull_chunk(timeout=0.0) # grab from LSL\n",
    "\n",
    "        np_eo.extend(chunk_data) # add to our list\n",
    "        w.update(chunk_data) # update the GUI window        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=16, n_times=2788\n",
      "    Range : 0 ... 2787 =      0.000 ...    22.296 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "# and we make the MNE data file from it\n",
    "raw_eo = mne.io.RawArray(np.transpose(np_eo)*1E-6,\n",
    "                        mne.create_info(channel_names, \n",
    "                                    sampling_freq, \n",
    "                                    'eeg', \n",
    "                                    'standard_1020',\n",
    "                                    'WARNING'),\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the eo data  (run1)\n",
    "- scroll through the data see if the EEG signal is what you expect\n",
    "- mark bad channels (that cannot be rescued)\n",
    "- mark bad segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a simple display filter\n",
    "raw_eo.filter(1, 35, verbose='WARNING').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save this raw data to disk (filename taken care of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file.\n",
      "Writing /home/johan/nf/rawdata/BrainTraining/bids/sub-01/sess-01/eeg/sub-01_sess-01_task-eo_run-01.fif\n",
      "Closing /home/johan/nf/rawdata/BrainTraining/bids/sub-01/sess-01/eeg/sub-01_sess-01_task-eo_run-01.fif [done]\n"
     ]
    }
   ],
   "source": [
    "this_raw_fname = 'sub-{:02d}_sess-{:02d}_task-{}_run-{:02d}.fif'.format(sub, sess, 'eo', run)\n",
    "fname_raw_eo_run1 = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "raw_eo.save(fname_raw_eo_run1, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Some Data (Eye Closed)\n",
    "- Inform your subject to keep the eye closed, then start the measurement\n",
    "- duration about 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put the incoming data in here:\n",
    "np_ec = dynarray.DynamicArray((None, len(channel_names)))\n",
    "\n",
    "# init/open the window in which we visualize data\n",
    "w=guis.AcquireData(sampling_freq, channel_names)\n",
    "\n",
    "# before we start, pull everything from the buffer (empty it)\n",
    "data_inlet.pull_chunk()\n",
    "while data_inlet.samples_available(): data_inlet.pull_chunk() \n",
    "    \n",
    "# then start acquiring data as long as button 'stop' not pressed:\n",
    "w.RUNLOOP=True\n",
    "while w.RUNLOOP:\n",
    "\n",
    "    if not data_inlet.samples_available():\n",
    "        w.update(None)\n",
    "    else:\n",
    "        \n",
    "        chunk_data, chunk_times = data_inlet.pull_chunk(timeout=0.0) # grab from LSL\n",
    "\n",
    "        np_ec.extend(chunk_data) # add to our list\n",
    "        w.update(chunk_data) # update the GUI window        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=16, n_times=2247\n",
      "    Range : 0 ... 2246 =      0.000 ...    17.968 secs\n",
      "Ready.\n",
      "setting bad chanels in the data set to be the same as previous dataset\n",
      "['O1', 'O2', 'C3', 'T5', 'Fp1', 'P4']\n",
      "Do not change bad channels further - re-do eo and ec if needed!\n"
     ]
    }
   ],
   "source": [
    "# and we make the MNE data file from it        \n",
    "raw_ec = mne.io.RawArray(np.transpose(np_ec)*1E-6,\n",
    "                        mne.create_info(channel_names, \n",
    "                                    sampling_freq, \n",
    "                                    'eeg', \n",
    "                                    'standard_1020',\n",
    "                                    'WARNING')\n",
    "                       )\n",
    "raw_ec.info['bads'] = raw_eo.info['bads']\n",
    "print('setting bad chanels in the data set to be the same as previous dataset')\n",
    "print(raw_ec.info['bads'])\n",
    "print('Do not change bad channels further - re-do eo and ec if needed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the ec data\n",
    "- scroll through the data see if the EEG signal is what you expect\n",
    "- mark bad channels (that cannot be rescued)\n",
    "- mark bad segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a simple display filter\n",
    "raw_ec.filter(1, 35, verbose='WARNING').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save this raw data to disk (filename taken care of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing file.\n",
      "Writing /home/johan/nf/rawdata/BrainTraining/bids/sub-01/sess-01/eeg/sub-01_sess-01_task-ec_run-01.fif\n",
      "Closing /home/johan/nf/rawdata/BrainTraining/bids/sub-01/sess-01/eeg/sub-01_sess-01_task-ec_run-01.fif [done]\n"
     ]
    }
   ],
   "source": [
    "this_raw_fname = 'sub-{:02d}_sess-{:02d}_task-{}_run-{:02d}.fif'.format(sub, sess, 'ec', run)\n",
    "fname_raw_ec_run = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "raw_ec.save(fname_raw_ec_run, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle the Ocular Artifacts\n",
    "- this will automatically read in the EO data  (run1)\n",
    "- and run the ICA analysis\n",
    "- your job is to select the component most resembling ocular artifact\n",
    "- and the matrix will/should be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O1', 'O2', 'C3', 'T5', 'Fp1', 'P4']\n",
      "21, 22\n",
      "channels to remove: [0, 2, 4, 6, 7, 15]\n",
      "channel mask: [False, True, False, True, False, True, False, False, True, True, True, True, True, True, True, False]\n",
      "number of samples to remove: 189\n",
      "raw original shape: 16, 2788\n",
      "np new shape: 2599, 10\n",
      "apply filter: 3 to 45\n",
      "Dropped 29 outliers\n",
      "Creating RawArray with float64 data, n_channels=10, n_times=2570\n",
      "    Range : 0 ... 2569 =      0.000 ...    20.552 secs\n",
      "Ready.\n",
      "Fitting ICA to data using 10 channels (please be patient, this may take a while)\n",
      "Inferring max_pca_components from picks\n",
      "Using all PCA components: 10\n",
      "Computing Extended Infomax ICA\n",
      "Fitting ICA took 1.3s.\n",
      "ICA/CSP time elapsed = 1.370880126953125s\n",
      "Table drawing time elapsed = 2.414292573928833s\n",
      "Created an ICA Spatial Filter\n",
      "<pynfb.signal_processing.filters.SpatialRejection object at 0x7f5b50de2cc0>\n"
     ]
    }
   ],
   "source": [
    "# run the ICA calculation; analysis taken from 8_1_SingleRun.ipynb\n",
    "# extracting the ch names and time information\n",
    "sampling_freq = raw_eo.info['sfreq']\n",
    "bad_channels = raw_eo.info['bads']\n",
    "bad_segments = [(a['onset'], a['duration']) for a in raw_eo.annotations if re.search('BAD', a['description'])]\n",
    "print(bad_channels)\n",
    "for s in bad_segments: print('%.2g, %.2g' % (s[0], sum(s)))\n",
    "\n",
    "# convert to ch mask; and samples to keep/remove\n",
    "bad_channel_indices = [i for i, ch in enumerate(raw_eo.info['ch_names'])  if ch in bad_channels]\n",
    "bad_channel_mask = [ch not in bad_channels for ch in raw_eo.ch_names]\n",
    "\n",
    "# display some information\n",
    "[range(int(b * sampling_freq), int((b+d) * sampling_freq)) for b, d in bad_segments]\n",
    "\n",
    "if bad_segments:\n",
    "    bad_segment_samples = np.concatenate([range(int(b * sampling_freq), int((b+d) * sampling_freq)) for b, d in bad_segments])\n",
    "else:\n",
    "    bad_segment_samples=[]\n",
    "\n",
    "print('channels to remove: %s' % bad_channel_indices)\n",
    "print('channel mask: ' + repr(bad_channel_mask))\n",
    "print('number of samples to remove: %s' % len(bad_segment_samples))\n",
    "\n",
    "# remove the bad channels and bad samples, make new data matrices:\n",
    "np_eo_forica = raw_eo.copy().get_data().T\n",
    "\n",
    "np_eo_forica = np.delete(np_eo_forica, bad_segment_samples, axis=0)\n",
    "np_eo_forica = np.delete(np_eo_forica, bad_channel_indices, axis=1)\n",
    "\n",
    "print('raw original shape: %d, %d' % raw_eo.get_data().shape)\n",
    "print('np new shape: %d, %d' % np_eo_forica.shape)\n",
    "\n",
    "# run the ICA analysis:\n",
    "from pynfb.protocols.ssd.topomap_selector_ica import ICADialog\n",
    "\n",
    "ica_rejection, _, _, ica_unmixing_matrix, _, _ = ICADialog.get_rejection(\n",
    "    np_eo_forica, \n",
    "    [n for n in raw_eo.ch_names if n not in bad_channels], \n",
    "    sampling_freq,\n",
    "    decomposition=None\n",
    ")\n",
    "\n",
    "ica_rejection = ica_rejection.expand_by_mask(bad_channel_mask)\n",
    "\n",
    "print('Created an ICA Spatial Filter')\n",
    "print(ica_rejection)\n",
    "\n",
    "\n",
    "\n",
    "# loading the ICA:\n",
    "# with open(fname_ica_ocular_rejection, 'rb') as f: ica_rejection = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the ocular Rejection\n",
    "- close all open windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-01/sess-01/eeg/sub-01_sess-01_ica-ocular-rejection_run-01.pkl\n"
     ]
    }
   ],
   "source": [
    "# we save the ICA for ocular rejection:\n",
    "# save the ICA rejection \n",
    "this_raw_fname = 'sub-{:02d}_sess-{:02d}_ica-ocular-rejection_run-{:02d}.pkl'.format(sub, sess, run)\n",
    "fname_ica_ocular_rejection = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "with open(fname_ica_ocular_rejection, 'wb') as f: pickle.dump(ica_rejection, f)\n",
    "print('saved: ' + fname_ica_ocular_rejection)\n",
    "\n",
    "# save it also as a .txt matrix (for matlab)\n",
    "np.savetxt(re.sub('.pkl$','.txt', fname_ica_ocular_rejection), ica_rejection.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work On Alpha Power Suppression with CSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O1', 'O2', 'C3', 'T5', 'Fp1', 'P4']\n",
      "[0, 2, 4, 6, 7, 15]\n",
      "21, 22\n",
      "17, 18\n",
      "189 bad samples in eyes open\n",
      "165 bad samples in eyes closed\n"
     ]
    }
   ],
   "source": [
    "# - load in dataset ec\n",
    "\n",
    "# check bad channels in eo, compare to ec\n",
    "if not raw_ec.info['bads'] == raw_eo.info['bads']:\n",
    "    raise Exception('Bad Channels are not the same between the two datasets - fix this first')\n",
    "\n",
    "# sort out the bad channels and bad segments over EO and EC data we recorded earlier:\n",
    "print(bad_channels)\n",
    "bad_channel_indices = [i for i, ch in enumerate(raw_eo.info['ch_names'])  if ch in bad_channels]\n",
    "print(bad_channel_indices)\n",
    "bad_channel_mask = [ch not in bad_channels for ch in raw_eo.ch_names]\n",
    "\n",
    "bad_segments_eo = [(a['onset'], a['duration']) for a in raw_eo.annotations if re.search('BAD', a['description'])]\n",
    "for s in bad_segments_eo: print('%.2g, %.2g' % (s[0], sum(s)))\n",
    "bad_segments_ec = [(a['onset'], a['duration']) for a in raw_ec.annotations if re.search('BAD', a['description'])]\n",
    "for s in bad_segments_ec: print('%.2g, %.2g' % (s[0], sum(s)))\n",
    "\n",
    "if bad_segments_eo:\n",
    "    bad_segment_samples_eo = np.concatenate([range(int(b * sampling_freq), int((b+d) * sampling_freq)) for b, d in bad_segments_eo])\n",
    "else:\n",
    "    bad_segment_samples_eo=[]\n",
    "if bad_segments_ec:\n",
    "    bad_segment_samples_ec = np.concatenate([range(int(b * sampling_freq), int((b+d) * sampling_freq)) for b, d in bad_segments_ec])\n",
    "else:\n",
    "    bad_segment_samples_ec=[]\n",
    "print('{} bad samples in eyes open'.format(len(bad_segment_samples_eo)))    \n",
    "print('{} bad samples in eyes closed'.format(len(bad_segment_samples_ec)))   \n",
    "\n",
    "# copy, remove bad samples, apply ocular rejection, remove bad channels:\n",
    "np_eo_for_csp = raw_eo.copy().filter(3, 45, verbose='WARNING').get_data().T\n",
    "np_eo_for_csp = np.delete(np_eo_for_csp, bad_segment_samples, axis=0)\n",
    "np_eo_for_csp = ica_rejection.apply(np_eo_for_csp)\n",
    "np_eo_for_csp = np.delete(np_eo_for_csp, bad_channel_indices, axis=1)\n",
    "\n",
    "np_ec_for_csp = raw_ec.copy().filter(3, 45, verbose='WARNING').get_data().T\n",
    "np_ec_for_csp = np.delete(np_ec_for_csp, bad_segment_samples, axis=0)\n",
    "np_ec_for_csp = ica_rejection.apply(np_ec_for_csp)\n",
    "np_ec_for_csp = np.delete(np_ec_for_csp, bad_channel_indices, axis=1)\n",
    "\n",
    "# concatenate it for entry into CSP removal (alpha power)\n",
    "data_for_csp = np.vstack((np_eo_for_csp, np_ec_for_csp))\n",
    "\n",
    "# make the 'labels' vector: 0 for length of eo; 1 for length of ec.\n",
    "n_timepoints_in_eo = np_eo_for_csp.shape[0]\n",
    "n_timepoints_in_ec = np_ec_for_csp.shape[0]\n",
    "labels_for_csp = np.hstack((np.zeros(n_timepoints_in_eo), np.ones(n_timepoints_in_ec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply filter: 3 to 45\n",
      "Dropped 28 outliers\n",
      "ICA/CSP time elapsed = 0.05384397506713867s\n",
      "Table drawing time elapsed = 2.5578622817993164s\n"
     ]
    }
   ],
   "source": [
    "# bring up the GUI for CSP filtering of the data:\n",
    "csp_rejection, filter, topography, _, bandpass, to_all = ICADialog.get_rejection(\n",
    "    data_for_csp,\n",
    "    [n for n in raw_eo.ch_names if n not in bad_channels], \n",
    "    sampling_freq,\n",
    "    mode='csp', \n",
    "    _stimulus_split=False,\n",
    "    labels=labels_for_csp, # will convert to 0-1 vector for each sample in x\n",
    "    marks=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the ocular Rejection\n",
    "- close all open windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/johan/nf/rawdata/BrainTraining/bids/sub-01/sess-01/eeg/sub-01_sess-01_csp-alpha-rejection_run-01.pkl\n"
     ]
    }
   ],
   "source": [
    "# we save the ICA for ocular rejection:\n",
    "# save the ICA rejection \n",
    "this_raw_fname = 'sub-{:02d}_sess-{:02d}_csp-alpha-rejection_run-{:02d}.pkl'.format(sub, sess, run)\n",
    "fname_csp_alpha_rejection = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "with open(fname_csp_alpha_rejection, 'wb') as f: pickle.dump(csp_rejection, f)\n",
    "print('saved: ' + fname_csp_alpha_rejection)\n",
    "\n",
    "# save it also as .txt matrix (for matlab)\n",
    "np.savetxt(re.sub('.pkl$','.txt', fname_csp_alpha_rejection), csp_rejection.val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurofeedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start up the Stimulus on Computer\n",
    "- connect via WiFi\n",
    "    - ip address = 10.42.0.1\n",
    "    - host = stim-pc\n",
    "    - password = PASSWORD, OR 12345678\n",
    "- ideally this should already be up and running, so you can skip over these more fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callpyff import bcinetwork, bcixml\n",
    "bcinet = bcinetwork.BciNetwork('10.42.0.1', bcinetwork.FC_PORT, bcinetwork.GUI_PORT, 'bcixml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TestD2', 'MovingRhomb', 'EOEC', 'LibetClock', 'BrainWaveTraining_II', 'TobiQLAdapter', 'VisualOddballVE', 'EyetrackerFeedback', 'HexoSpeller', 'P300_Rectangle', 'ERPHex', 'BrainWaveTraining', 'StopVigilanceTask', 'FeedbackCursorArrow', 'TrivialPong', 'CheckerboardVEP', 'HexoSpellerVE', 'BoringClock', 'nback_verbal', 'Lesson01', 'BrainPong', 'CakeSpellerVE', 'MovingRhombGL', 'RestingState', 'NFBasicThermometer', 'RSVPSpeller', 'EEGfMRILocalizer', 'MultiVisualOddball', 'Lesson01b', 'GoalKeeper', 'CenterSpellerVE', 'Oddball', 'EyetrackerRawdata', 'StroopFeedback', 'ERPMatrix', 'Lesson04', 'Lesson05', 'Lesson06', 'VisualOddball', 'Lesson02', 'Lesson03']\n"
     ]
    }
   ],
   "source": [
    "print(bcinet.getAvailableFeedbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcinet.send_init('BrainWaveTraining_II')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the stimulus about the monitor\n",
    "bcinet.send_signal(bcixml.BciSignal({'EX_TESTNFNOISE': False},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'MONITOR_PIXWIDTH': 1366},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'MONITOR_PIXHEIGHT': 768},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'MONITOR_FULLSCR': True},None, bcixml.INTERACTION_SIGNAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nftools.nftools import signaltracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr: 5.00, dur: 0.20\n",
      "bcinet is passed on\n",
      "thr: 250.00, dur: 0.15\n",
      "bcinet is passed on\n"
     ]
    }
   ],
   "source": [
    "# these objects can send variables over to the stimulus\n",
    "# parameters to convert the filtered EEG signal to the stimulus\n",
    "# the following parameter should come out of the EEG data, as our initial threshold to use:\n",
    "global_std_band=5\n",
    "\n",
    "from nftools.nftools import signaltracking\n",
    "track_for_eeg_stimuli = signaltracking.sending_to_nfstim(\n",
    "    sampling_freq,\n",
    "    thr=1.0 * global_std_band, \n",
    "    dur=0.20, \n",
    "    feedback_type='eeg', \n",
    "    max4audio=1.2, \n",
    "    bcinet=bcinet, \n",
    "    st_scaling=5 * global_std_band,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# parameters to convert the filtered EMG signal to the stimulus\n",
    "track_for_emg_stimuli = signaltracking.sending_to_nfstim(\n",
    "    sampling_freq,\n",
    "    thr=50 * global_std_band, \n",
    "    dur=0.15, \n",
    "    feedback_type='emg', \n",
    "    bcinet=bcinet, \n",
    "    st_scaling=100 * global_std_band,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the process that will monitor events from the Stimulation Laptop\n",
    " - this will listen on UDP port 6500 for any incoming markers\n",
    " - This is to convert signals from the Presentation into annotations\n",
    " - grab markers in the NF loop with: `while not marker_queue.empty():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3.97.246:6500\n"
     ]
    }
   ],
   "source": [
    "# tell the stimulus computer where markers need to be sent to:\n",
    "import socket\n",
    "ip_address = socket.gethostbyname(socket.getfqdn())\n",
    "port = 6500\n",
    "print(ip_address+\":\"+str(port))\n",
    "bcinet.send_signal(bcixml.BciSignal({'EVENT_destip': ip_address},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'EVENT_destport': port},None, bcixml.INTERACTION_SIGNAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting markerserver on port: 6500\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue, Event\n",
    "import time\n",
    "if not 'marker_queue' in locals():\n",
    "    marker_queue = Queue()\n",
    "if not 'stop_server' in locals():\n",
    "    stop_server = Event()\n",
    "from libmushu.ampdecorator import marker_reader\n",
    "\n",
    "stop_server.set()\n",
    "time.sleep(0.5)\n",
    "stop_server.clear()\n",
    "\n",
    "tcp_reader = Process(target=marker_reader, args=(\n",
    "    marker_queue,\n",
    "    Event().set(),\n",
    "    Event(),\n",
    "    stop_server,\n",
    "    6500\n",
    "    )\n",
    ")\n",
    "\n",
    "tcp_reader.start()\n",
    "# stop the server with: \n",
    "# stop_server.set()\n",
    "# tcp_reader.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the NF Stimulation\n",
    " - you still have to press <ENTER> to actually start the stimulus, but Not Yet!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bcinet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1aec7b6a2a63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbcinet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bcinet' is not defined"
     ]
    }
   ],
   "source": [
    "bcinet.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the Real-Time analysis Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynfb.signal_processing.filters import (FilterSequence, \n",
    "                                             CFIRBandEnvelopeDetector, \n",
    "                                             ExponentialSmoother,\n",
    "                                             SpatialFilter,\n",
    "                                             ButterFilter,\n",
    "                                             ButterBandEnvelopeDetector,\n",
    "                                             ScalarButterFilter,\n",
    "                                             MASmoother,\n",
    "                                             FFTBandEnvelopeDetector,\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Filter Sequence for NF\n",
    "\n",
    "# Which channel do we select for the EEG - we do C3.\n",
    "rt_eeg_channels = ['C3']\n",
    "rt_eeg_channels_mask = np.where([ch in rt_eeg_channels for ch in channel_names], 1, 0)/len(rt_eeg_channels)\n",
    "\n",
    "preprocess_filters_eeg = FilterSequence([\n",
    "    ica_rejection,\n",
    "    csp_rejection,\n",
    "    SpatialFilter(rt_eeg_channels_mask),\n",
    "])\n",
    "envelope_filter_eeg = CFIRBandEnvelopeDetector([12, 15], sampling_freq, MASmoother(150))\n",
    "butter_visualization_eeg = ButterFilter([12, 15], sampling_freq, 1)\n",
    "\n",
    "\n",
    "# Processing of the EMG - this is basically our second channel...\n",
    "rt_emg_channels = ['T3','T4','Fp1','Fp2']\n",
    "rt_emg_channels_mask = np.where([ch in rt_emg_channels for ch in channel_names], 1, 0)/len(rt_emg_channels)\n",
    "preprocess_filters_emg = FilterSequence([\n",
    "    SpatialFilter(rt_eeg_channels_mask),\n",
    "])\n",
    "envelope_filter_emg = ButterBandEnvelopeDetector([55, 56], sampling_freq, MASmoother(150))\n",
    "butter_visualization_emg = ButterFilter([55, 65], sampling_freq, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our UI 'Experience' -- it can consist of 3 separate, movable windows (for now)\n",
    "# same window as before + 2 other windows - 1 for interaction with stim/thresholds; 1 for looking\n",
    "# at the analysis itself.\n",
    "w_acquire = guis.AcquireData(sampling_freq, channel_names)\n",
    "w_interaction = guis.NFChangeThresholds(track_for_eeg_stimuli, track_for_emg_stimuli)\n",
    "w_eeganalysis = guis.AnalyzeData(sampling_freq, ['EEG','env','thr','vmarker','amarker'],track_for_eeg_stimuli)\n",
    "w_emganalysis = guis.AnalyzeData(sampling_freq, ['EMG','env','thr','vmarker','amarker'],track_for_eeg_stimuli)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Press start on the NF Stimulation Laptop now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear EEG Data buffer\n",
    "data_inlet.pull_chunk()\n",
    "while data_inlet.samples_available(): data_inlet.pull_chunk() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# containers for data collection: \n",
    "time_nf = dynarray.DynamicArray()\n",
    "data_nf = dynarray.DynamicArray((None, len(channel_names)))\n",
    "data_analysis_eeg = dynarray.DynamicArray((None, 5))\n",
    "data_analysis_emg = dynarray.DynamicArray((None, 5))\n",
    "\n",
    "# collect markers from the Stimulation:\n",
    "stim_Annotations = mne.Annotations(0, 0, 'Start NF Loop')\n",
    "\n",
    "# collect markers from the interaction GUI:\n",
    "w_interaction.GUI_Annotations.append(time.time()-w_interaction.begin_time, 0, 'startloop')\n",
    "\n",
    "# start the loop\n",
    "w_acquire.RUNLOOP=True\n",
    "acquisition_start = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "while w_acquire.RUNLOOP:\n",
    "    \n",
    "    # check if the presentation gave any markers, and collect them\n",
    "    while not marker_queue.empty():\n",
    "        stim_time, stim_code = marker_queue.get()\n",
    "        stim_Annotations.append(stim_time, 0, stim_code)\n",
    "    \n",
    "    \n",
    "    if not data_inlet.samples_available(): \n",
    "        w_acquire.update(None)\n",
    "    else:\n",
    "        chunk_data, chunk_times = data_inlet.pull_chunk() # grab from LSL\n",
    "\n",
    "        # update signal window so we can see raw signals\n",
    "        w_acquire.update(chunk_data) \n",
    "        \n",
    "        # store the raw data (and times)\n",
    "        time_nf.extend(chunk_times)\n",
    "        data_nf.extend(chunk_data)\n",
    "\n",
    "        # store the markers of the Stimulus computer\n",
    "        while not marker_queue.empty():\n",
    "            stim_time, stim_code = marker_queue.get()\n",
    "            stim_Annotations.append(stim_time, 0, stim_code)\n",
    "\n",
    "        \n",
    "        #\n",
    "        # EEG Signal processing\n",
    "        #\n",
    "        \n",
    "        # apply spatial and temporal filters to the raw signal; for EEG and EMG:\n",
    "        preprocessed_eeg = preprocess_filters_eeg.apply(chunk_data)        \n",
    "        envelope_eeg = envelope_filter_eeg.apply(preprocessed_eeg)\n",
    "\n",
    "\n",
    "        # check if it's above threshold or not; send markers to stimulus computer\n",
    "        visual_markers_eeg, audio_markers_eeg = track_for_eeg_stimuli.check_above_threshold(envelope_eeg)  \n",
    "        # send the signal  to stimulus computer, too\n",
    "        track_for_eeg_stimuli.send_data_signal(envelope_eeg) \n",
    "        \n",
    "\n",
    "        # visualize the processing steps\n",
    "        analysis_names_eeg = ('EEG', 'env', 'thr', 'vmarker', 'amarker')\n",
    "        analysis_data_eeg = np.vstack((\n",
    "            np.abs(butter_visualization_eeg.apply(preprocessed_eeg[:, None])[:, 0]),\n",
    "            envelope_eeg,\n",
    "            track_for_eeg_stimuli.thr * np.ones(preprocessed_eeg.shape),\n",
    "            visual_markers_eeg,\n",
    "            audio_markers_eeg,\n",
    "        )).T\n",
    "        \n",
    "        data_analysis_eeg.extend(analysis_data_eeg)  # store it for later conversion\n",
    "        w_eeganalysis.update(analysis_data) # update analysis window\n",
    "\n",
    "        \n",
    "        #\n",
    "        # EMG Signal Processing\n",
    "        #\n",
    "        \n",
    "        # do the same for the EMG NF, too:\n",
    "        # apply spatial and temporal filters to the raw signal; for EEG and EMG:\n",
    "        preprocessed_emg = preprocess_filters_emg.apply(chunk_data)\n",
    "        envelope_emg = envelope_filter_emg.apply(preprocessed_emg)\n",
    "        \n",
    "        # check if it's above threshold or not; send markers to stimulus computer\n",
    "        visual_markers_emg, audio_markers_emg = track_for_emg_stimuli.check_above_threshold(envelope_emg) \n",
    "        # send the signal  to stimulus computer, too\n",
    "        track_for_emg_stimuli.send_data_signal(envelope_emg) \n",
    "\n",
    "        # visualize the processing steps\n",
    "        analysis_names_eeg = ('EMG', 'env', 'thr', 'vmarker', 'amarker')\n",
    "        analysis_data_emg = np.vstack((\n",
    "            np.abs(butter_visualization_emg.apply(preprocessed_emg[:, None])[:, 0]),\n",
    "            envelope_emg,\n",
    "            track_for_eeg_stimuli.thr * np.ones(preprocessed_emg.shape),\n",
    "            visual_markers_emg,\n",
    "            audio_markers_emg,\n",
    "        )).T\n",
    "        \n",
    "        data_analysis_emg.extend(analysis_data)  # store it for later conversion\n",
    "        w_emganalysis.update(analysis_data) # update analysis window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save all the gathered data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=16, n_times=2247\n",
      "    Range : 0 ... 2246 =      0.000 ...    17.968 secs\n",
      "Ready.\n",
      "setting bad chanels in the data set to be the same as previous dataset\n",
      "['O1', 'O2', 'C3', 'T5', 'Fp1', 'P4']\n",
      "Do not change bad channels further - re-do eo and ec if needed!\n"
     ]
    }
   ],
   "source": [
    "# and we make the MNE data file from it        \n",
    "raw_nftraining = mne.io.RawArray(np.transpose(np_ec)*1E-6,\n",
    "                        mne.create_info(channel_names, \n",
    "                                    sampling_freq, \n",
    "                                    'eeg', \n",
    "                                    'standard_1020',\n",
    "                                    'WARNING')\n",
    "                       )\n",
    "# saving the raw data\n",
    "this_raw_fname = 'sub-{:02d}_sess-{:02d}_task-{}_run-{:02d}.fif'.format(sub, sess, 'raw-nftraining', run)\n",
    "fname_raw_nftraining_run = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "raw_nftraining.save(fname_raw_ec_run, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=5, n_times=10108\n",
      "    Range : 0 ... 10107 =      0.000 ...    20.214 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "# and we make the MNE data file from it\n",
    "rtanalyzed_nftraining_eeg = mne.io.RawArray(np.transpose(np.array(analysis_names_eeg) * [1E-6, 1E-6, 1E-6, 1, 1]),\n",
    "                        mne.create_info(analyzed_nftraining_eeg, \n",
    "                                    sampling_freq, \n",
    "                                    ['eeg','eeg','eeg','stim','stim'], \n",
    "                                    None)\n",
    "                       )\n",
    "this_raw_fname = 'sub-{:02d}_sess-{:02d}_task-{}_run-{:02d}.fif'.format(sub, sess, 'rtanalyzed-nftraining-eeg', run)\n",
    "fname_rtanalyzed_nftraining_eeg = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "\n",
    "# handle annotations, since w_interaction starts before data acquisition\n",
    "annots = w_interaction.GUI_Annotations.copy()\n",
    "tdelta = datetime.strptime(acquisition_start,'%Y-%m-%d %H:%M:%S.%f').timestamp() - annots.orig_time\n",
    "annots.onset -= tdelta\n",
    "annots.orig_time=None\n",
    "\n",
    "starting_annots = annots[list(map(lambda d: re.match('start.*', d) is not None, annots.description))]\n",
    "starting_annots.onset = 0.1*np.ones(len(starting_annots))\n",
    "starting_annots.orig_time = None\n",
    "\n",
    "# handle also these:\n",
    "# stim_Annotations ... process them\n",
    "\n",
    "\n",
    "rtanalyzed_nftraining_eeg.set_annotations(starting_annots + annots)\n",
    "rtanalyzed_nftraining_eeg.save(fname_rtanalyzed_nftraining_eeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we make the MNE data file from it\n",
    "analyzed_nftraining_emg = mne.io.RawArray(np.transpose(np.array(analysis_names_emg) * [1E-6, 1E-6, 1E-6, 1, 1]),\n",
    "                        mne.create_info(analysis_names_emg, \n",
    "                                    sampling_freq, \n",
    "                                    ['emg','emg','emg','stim','stim'], \n",
    "                                    None)\n",
    "                       )\n",
    "this_raw_fname = 'sub-{:02d}_sess-{:02d}_task-{}_run-{:02d}.fif'.format(sub, sess, 'nftraining-emg', run)\n",
    "fname_analyzed_nftraining_emg = os.path.join(this_save_dir, this_raw_fname)\n",
    "\n",
    "analyzed_nftraining_eeg.save(fname_analyzed_nftraining_emg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
