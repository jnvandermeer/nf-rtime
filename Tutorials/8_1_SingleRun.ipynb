{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nftools import guis\n",
    "import pylsl\n",
    "import time\n",
    "import numpy as np\n",
    "import mne, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%matplotlib qt  \n",
    "# %gui qt\n",
    "import warnings; warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the EEG Cap\n",
    "\n",
    "- fix the EEG Cap\n",
    "- run the `python raw_eo_data --stream`, followed by `/start`\n",
    "- (re)-start the Cap or USB if it doesn't work, followed by commands above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name = openbci_eeg\n",
      "sampling_freq = 125\n",
      "channel_count = 16\n",
      "channel_format = 1\n"
     ]
    }
   ],
   "source": [
    "# make the connection to the EEG Net (EEG Data)\n",
    "data_stream=pylsl.resolve_byprop(\"name\", \"openbci_eeg\", timeout=5.0)\n",
    "if data_stream:\n",
    "    data_inlet=pylsl.stream_inlet(data_stream[0])\n",
    "    stream_info = data_inlet.info()\n",
    "    stream_Fs = stream_info.nominal_srate()\n",
    "    stream_xml = stream_info.desc()\n",
    "    chans_xml = stream_xml.child(\"channels\")\n",
    "    chan_xml_list = []\n",
    "    ch = chans_xml.child(\"channel\")\n",
    "    while ch.name() == \"channel\":\n",
    "        chan_xml_list.append(ch)\n",
    "        ch = ch.next_sibling(\"channel\")\n",
    "    channel_names = [ch_xml.child_value(\"label\") for ch_xml in chan_xml_list]\n",
    "    data_inlet_dt = data_inlet.time_correction(timeout=5.0)\n",
    "    sampling_freq = data_stream[0].nominal_srate()\n",
    "    print('name = %s' % data_stream[0].name())\n",
    "    print('sampling_freq = %d' % sampling_freq)\n",
    "    print('channel_count = %d' % data_stream[0].channel_count())\n",
    "    print('channel_format = %d' % data_stream[0].channel_format())\n",
    "else:\n",
    "    raise Exception('No Data Stream Found - Is your EEG Cap running?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get (~ 2-3 minutes) of Eyes Open data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear EEG Data buffer\n",
    "while data_inlet.samples_available(): data_inlet.pull_chunk() \n",
    "\n",
    "# collect some data    \n",
    "np_eo = []\n",
    "w=guis.AcquireData(sampling_freq, channel_names)\n",
    "while w.RUNLOOP:\n",
    "    chunk_data, chunk_times = data_inlet.pull_chunk() # grab from LSL\n",
    "    if chunk_data:  # if there is any data\n",
    "        np_eo.append(chunk_data) # add to our list\n",
    "        w.update(chunk_data) # update the GUI window\n",
    "\n",
    "# and we make the MNE data file from it        \n",
    "raw_eo = mne.io.RawArray(np.concatenate(np_eo).T,\n",
    "                        mne.create_info(channel_names, \n",
    "                                    sampling_freq, \n",
    "                                    'eeg', \n",
    "                                    None)\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Bad Channels and Bad Time Intervals\n",
    "- Use the MNE Interface to click channels (those are bad)\n",
    "- Use also the MNE Interface ('a' button) to mark/drag\n",
    "bad segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "if 'w' in locals(): del(w)\n",
    "raw_eo.plot(scalings='auto');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "channels to remove: []\n",
      "channel mask: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "number of samples to remove: 0\n",
      "raw original shape: 16, 941\n",
      "np new shape: 941, 16\n",
      "apply filter: 3 to 45\n",
      "Dropped 141 outliers\n",
      "ICA/CSP time elapsed = 0.8720769882202148s\n",
      "Table drawing time elapsed = 4.004317283630371s\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'expand_by_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a5c54c1596e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mica_rejection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mica_rejection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_by_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_channel_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Created an ICA Spatial Filter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'expand_by_mask'"
     ]
    }
   ],
   "source": [
    "# Analysis\n",
    "# extracting the ch names and time information\n",
    "bad_channels = raw_eo.info['bads']\n",
    "bad_segments = [(a['onset'], a['duration']) for a in raw_eo.annotations if re.search('BAD', a['description'])]\n",
    "print(bad_channels)\n",
    "for s in bad_segments: print('%.2g, %.2g' % (s[0], sum(s)))\n",
    "\n",
    "# convert to ch mask; and samples to keep/remove\n",
    "bad_channel_indices = [i for i, ch in enumerate(raw_eo.info['ch_names']) \n",
    "                    if ch in bad_channels]\n",
    "bad_channel_mask = [ch not in bad_channels for ch in channel_names]\n",
    "\n",
    "[range(int(b * sampling_freq), int((b+d) * sampling_freq)) for b, d in bad_segments]\n",
    "\n",
    "if bad_segments:\n",
    "    bad_segment_samples = np.concatenate([range(int(b * sampling_freq), int((b+d) * sampling_freq)) for b, d in bad_segments])\n",
    "else:\n",
    "    bad_segment_samples=[]\n",
    "\n",
    "print('channels to remove: %s' % bad_channel_indices)\n",
    "print('channel mask: ' + repr(bad_channel_mask))\n",
    "print('number of samples to remove: %s' % len(bad_segment_samples))\n",
    "\n",
    "# remove the bad channels and bad samples, make new data matrices:\n",
    "np_eo_forica = raw_eo.copy().get_data().T\n",
    "\n",
    "np_eo_forica = np.delete(np_eo_forica, bad_segment_samples, axis=0)\n",
    "np_eo_forica = np.delete(np_eo_forica, bad_channel_indices, axis=1)\n",
    "\n",
    "print('raw original shape: %d, %d' % raw_eo.get_data().shape)\n",
    "print('np new shape: %d, %d' % np_eo_forica.shape)\n",
    "\n",
    "# # inspect the new data matrix in the viewer\n",
    "# raw_eo_forica = mne.io.RawArray(np_eo_forica.T,\n",
    "#                         mne.create_info([n for n in channel_names if n not in bad_channels], \n",
    "#                                     sampling_freq, \n",
    "#                                     'eeg', \n",
    "#                                     None)\n",
    "#                        )\n",
    "\n",
    "# raw_eo_forica.plot(scalings='auto');\n",
    "\n",
    "from pynfb.protocols.ssd.topomap_selector_ica import ICADialog\n",
    "\n",
    "ica_rejection, _, _, ica_unmixing_matrix, _, _ = ICADialog.get_rejection(\n",
    "    np_eo_forica, \n",
    "    [n for n in channel_names if n not in bad_channels], \n",
    "    sampling_freq,\n",
    "    decomposition=None\n",
    ")\n",
    "\n",
    "ica_rejection = ica_rejection.expand_by_mask(bad_channel_mask)\n",
    "\n",
    "print('Created an ICA Spatial Filter')\n",
    "print(ica_rejection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start up the Stimulus on Computer\n",
    "- connect via WiFi\n",
    "    - ip address = 10.42.0.1\n",
    "    - host = stim-pc\n",
    "    - password = PASSWORD, OR 12345678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callpyff import bcinetwork, bcixml\n",
    "bcinet = bcinetwork.BciNetwork('10.42.0.1', bcinetwork.FC_PORT, bcinetwork.GUI_PORT, 'bcixml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TestD2', 'MovingRhomb', 'EOEC', 'LibetClock', 'BrainWaveTraining_II', 'TobiQLAdapter', 'VisualOddballVE', 'EyetrackerFeedback', 'HexoSpeller', 'P300_Rectangle', 'ERPHex', 'BrainWaveTraining', 'StopVigilanceTask', 'FeedbackCursorArrow', 'TrivialPong', 'CheckerboardVEP', 'HexoSpellerVE', 'BoringClock', 'nback_verbal', 'Lesson01', 'BrainPong', 'CakeSpellerVE', 'MovingRhombGL', 'RestingState', 'NFBasicThermometer', 'RSVPSpeller', 'EEGfMRILocalizer', 'MultiVisualOddball', 'Lesson01b', 'GoalKeeper', 'CenterSpellerVE', 'Oddball', 'EyetrackerRawdata', 'StroopFeedback', 'ERPMatrix', 'Lesson04', 'Lesson05', 'Lesson06', 'VisualOddball', 'Lesson02', 'Lesson03']\n"
     ]
    }
   ],
   "source": [
    "print(bcinet.getAvailableFeedbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcinet.send_init('BrainWaveTraining_II')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcinet.send_signal(bcixml.BciSignal({'EX_TESTNFNOISE': False},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'MONITOR_PIXWIDTH': 1366},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'MONITOR_PIXHEIGHT': 768},None, bcixml.INTERACTION_SIGNAL))\n",
    "bcinet.send_signal(bcixml.BciSignal({'MONITOR_FULLSCR': True},None, bcixml.INTERACTION_SIGNAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nftools.nftools import signaltracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nftools.nftools.signaltracking' from '/home/johan/nf/nf-rtime/nftools/nftools/signaltracking.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(signaltracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thr: 1.00, dur: 0.20\n",
      "bcinet is passed on\n",
      "thr: 10.00, dur: 0.15\n",
      "bcinet is passed on\n"
     ]
    }
   ],
   "source": [
    "# parameters to convert the filtered EEG signal to the stimulus\n",
    "from nftools.nftools import signaltracking\n",
    "track_for_eeg_stimuli = signaltracking.sending_to_nfstim(\n",
    "    sampling_freq,\n",
    "    thr=1.0, \n",
    "    dur=0.20, \n",
    "    feedback_type='eeg', \n",
    "    max4audio=1.2, \n",
    "    bcinet=bcinet, \n",
    "    st_scaling=10,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# parameters to convert the filtered EMG signal to the stimulus\n",
    "track_for_emg_stimuli = signaltracking.sending_to_nfstim(\n",
    "    sampling_freq,\n",
    "    thr=10, \n",
    "    dur=0.15, \n",
    "    feedback_type='emg', \n",
    "    bcinet=bcinet, \n",
    "    st_scaling=30,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tell the NF Laptop to start!\n",
    " - you still have to press <ENTER> to actually start the stimulus, but after starting the NF loop later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bcinet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1aec7b6a2a63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbcinet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bcinet' is not defined"
     ]
    }
   ],
   "source": [
    "bcinet.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the MarkerServer in the background\n",
    " - this will listen on UDP port 6500 for any incoming markers\n",
    " - This is to convert signals from the Presentation into annotations\n",
    " - grab markers in the NF loop with: `while not marker_queue.empty():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting markerserver on port: 6500\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue, Event\n",
    "import time\n",
    "if not 'marker_queue' in locals():\n",
    "    marker_queue = Queue()\n",
    "if not 'stop_server' in locals():\n",
    "    stop_server = Event()\n",
    "from libmushu.ampdecorator import marker_reader\n",
    "\n",
    "stop_server.set()\n",
    "time.sleep(0.5)\n",
    "stop_server.clear()\n",
    "\n",
    "tcp_reader = Process(target=marker_reader, args=(\n",
    "    marker_queue,\n",
    "    Event().set(),\n",
    "    Event(),\n",
    "    stop_server,\n",
    "    6500\n",
    "    )\n",
    ")\n",
    "\n",
    "tcp_reader.start()\n",
    "# stop the server with: \n",
    "# stop_server.set()\n",
    "# tcp_reader.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Real-Time EEG Signal Analysis during NF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynfb.signal_processing.filters import (FilterSequence, \n",
    "                                             CFIRBandEnvelopeDetector, \n",
    "                                             ExponentialSmoother,\n",
    "                                             SpatialFilter,\n",
    "                                             ButterFilter,\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Filter Sequence for NF\n",
    "\n",
    "# Which channel do we select for the EEG?\n",
    "rt_eeg_channels = ['C3']\n",
    "rt_eeg_channels_mask = np.where([ch in rt_eeg_channels for ch in channel_names], 1, 0)/len(rt_eeg_channels)\n",
    "\n",
    "preprocess_filters_eeg = FilterSequence([\n",
    "    ica_rejection,\n",
    "    SpatialFilter(rt_eeg_channels_mask),\n",
    "    ButterFilter([1, None], sampling_freq, 1),\n",
    "])\n",
    "envelope_filter_eeg = CFIRBandEnvelopeDetector([12, 15], sampling_freq, ExponentialSmoother(0.9))\n",
    "\n",
    "\n",
    "# which channel for the EMG (Muscle)?\n",
    "rt_emg_channels = ['T3', 'T4'] # check if they're not in the bad channels..\n",
    "rt_emg_channels_mask = np.where([ch in rt_eeg_channels for ch in channel_names], 1, 0)/len(rt_eeg_channels)\n",
    "\n",
    "preprocess_filters_emg = FilterSequence([\n",
    "    SpatialFilter(rt_emg_channels_mask),\n",
    "    ButterFilter([None, 50], sampling_freq, 1),\n",
    "])\n",
    "envelope_filter_emg = CFIRBandEnvelopeDetector([50, 60], sampling_freq, ExponentialSmoother(0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the experiment\n",
    " - grab the data, apply RT Analysis, send to stimulus computer\n",
    " - press Enter on the Stimulus Computer to get started\n",
    " - subject trans (or should train) ther EEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our UI 'Experience' -- it can consist of 3 separate, movable windows (for now)\n",
    "# same window as before + 2 other windows - 1 for interaction with stim/thresholds; 1 for looking\n",
    "# at the analysis itself.\n",
    "w_acquire = guis.AcquireData(sampling_freq, channel_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_interaction = guis.NFChangeThresholds(track_for_eeg_stimuli, track_for_eeg_stimuli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'guis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-41083bb70882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw_eeganalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mguis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCustomSignalViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'EEG'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'env'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'thr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'visual'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'audio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotch_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'guis' is not defined"
     ]
    }
   ],
   "source": [
    "w_eeganalysis = guis.CustomSignalViewer(sampling_freq, ['EEG','env','thr','visual','audio'], overlap=True, notch_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_eeganalysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_eeganalysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending signal -- I ! - 6 - 3 - True - -1\n",
      "sending signal! - 3 False - 1698.5897608752598 -- 0.2\n",
      "sending signal -- I ! - 4 - 7 - True - -1\n",
      "sending signal! - 7 False - 3115.7205400830403 -- 0.25\n",
      "sending signal! - 10 False - 7338.542447950324 -- 0.25\n",
      "sending signal! - 15 False - 13612.712324363889 -- 0.3\n",
      "sending signal! - 18 False - 16176.525104721888 -- 0.3\n",
      "sending signal! - 27 False - 20804.649195549086 -- 0.35\n",
      "sending signal! - 30 False - 22190.178177309404 -- 0.35\n",
      "sending signal! - 39 False - 29632.885221581822 -- 0.39999999999999997\n",
      "sending signal! - 42 False - 33096.352050780304 -- 0.39999999999999997\n",
      "sending signal! - 50 False - 44230.58759935846 -- 0.44999999999999996\n",
      "sending signal! - 54 False - 49065.492819553874 -- 0.44999999999999996\n",
      "sending signal! - 62 False - 62035.14703327439 -- 0.49999999999999994\n",
      "sending signal! - 64 False - 63504.37080444319 -- 0.49999999999999994\n",
      "sending signal! - 75 False - 83979.30517311534 -- 0.5499999999999999\n",
      "sending signal! - 75 False - 86092.17839093525 -- 0.5499999999999999\n",
      "sending signal! - 90 False - 106884.1125841625 -- 0.6\n",
      "sending signal! - 90 False - 106646.15653047658 -- 0.6\n",
      "sending signal! - 107 False - 124844.21279153846 -- 0.65\n",
      "sending signal! - 111 False - 127245.24830252836 -- 0.65\n",
      "sending signal! - 126 False - 145020.1409047548 -- 0.7000000000000001\n",
      "sending signal! - 132 False - 149389.50398027943 -- 0.7000000000000001\n",
      "sending signal! - 143 False - 161085.87312525345 -- 0.7500000000000001\n",
      "sending signal! - 145 False - 163044.61274418214 -- 0.7500000000000001\n",
      "sending signal! - 164 False - 184056.86690419004 -- 0.8000000000000002\n",
      "sending signal! - 168 False - 187724.7960111013 -- 0.8000000000000002\n",
      "sending signal! - 193 False - 211999.90961287767 -- 0.8500000000000002\n",
      "sending signal! - 195 False - 211828.18992269746 -- 0.8500000000000002\n",
      "sending signal! - 221 False - 234772.3948857645 -- 0.9000000000000002\n",
      "sending signal! - 222 False - 235663.91873084017 -- 0.9000000000000002\n",
      "sending signal! - 249 False - 259332.80315470672 -- 0.9500000000000003\n",
      "sending signal! - 250 False - 259158.3294782284 -- 0.9500000000000003\n",
      "sending signal! - 276 False - 292958.1587641728 -- 1.0000000000000002\n",
      "sending signal! - 278 False - 294411.80107170664 -- 1.0000000000000002\n",
      "sending signal! - 309 False - 322888.0292043932 -- 1.0500000000000003\n",
      "sending signal! - 310 False - 323632.52145642915 -- 1.0500000000000003\n",
      "sending signal! - 344 False - 370318.1831966745 -- 1.1000000000000003\n",
      "sending signal! - 345 False - 370990.7444063538 -- 1.1000000000000003\n",
      "sending signal! - 381 False - 403378.0235967692 -- 1.1500000000000004\n",
      "sending signal! - 383 False - 403888.47460087977 -- 1.1500000000000004\n"
     ]
    }
   ],
   "source": [
    "# clear EEG Data buffer\n",
    "while data_inlet.samples_available(): data_inlet.pull_chunk() \n",
    "\n",
    "# containers for data collection: \n",
    "time_nf = []\n",
    "data_nf = []\n",
    "data_analysis = []\n",
    "\n",
    "\n",
    "# collect markers (for annotations)\n",
    "stim_Annotations = mne.Annotations(0, 0, 'Start NF Stim')\n",
    "# the GUI ones will be in w_analysis.GUI_Annotations\n",
    "\n",
    "\n",
    "\n",
    "while w_acquire.RUNLOOP:\n",
    "    \n",
    "    # check of the presentation gave any markers, and collect them:\n",
    "    while not marker_queue.empty():\n",
    "        stim_time, stim_code = marker_queue.get()\n",
    "        stim_Annotations.append(stim_time, 0, stim_code)\n",
    "    \n",
    "    # process the data:\n",
    "    chunk_data, chunk_times = data_inlet.pull_chunk() # grab from LSL\n",
    "    if chunk_data:  # if there is any data\n",
    "\n",
    "        time_nf.append(chunk_times)\n",
    "        data_nf.append(chunk_data) # add to our list\n",
    "        \n",
    "        w_acquire.update(chunk_data) # update the GUI window\n",
    "        \n",
    "        \n",
    "        # check if anything comes from our bcinet; convert those to Annotations\n",
    "        # ... # for now; just a list - begin and end annotations are separate\n",
    "        # later we covert to timespans\n",
    "        # I don't really know whether bcinet can actually read from the stimulus??\n",
    "        # likely bcinet CAN also send, but exactly how to do this -- pudb on a running\n",
    "        # feedback instance.\n",
    "        #\n",
    "        # alternatively -- start up an LSL and send from fb and connect / receive from over here.\n",
    "        # I don't think this kind of use-case was readily though of by the pyff folks.\n",
    "        # but, I think we can try it out.\n",
    "        # we would need the PUDB in 'remote' mode to access and try this out.\n",
    "        # all my hard works in python bears fruit...\n",
    "        #\n",
    "        # in any case, here we should just check a queue - and if there's contents - we grow our \n",
    "        # annotations array.\n",
    "        \n",
    "        \n",
    "        # Apply Filters and draw in our second window\n",
    "\n",
    "        preprocessed_eeg = preprocess_filters_eeg.apply(chunk_data)\n",
    "        envelope_eeg = envelope_filter_eeg.apply(preprocessed_eeg)\n",
    "\n",
    "        preprocessed_emg = preprocess_filters_emg.apply(chunk_data)\n",
    "        envelope_emg = envelope_filter_eeg.apply(preprocessed_emg)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # interaction with the Stimulus Ciomputer -- this goes via the GUI's callbacks\n",
    "        \n",
    "        \n",
    "        visual_markers_eeg, audio_markers_eeg = track_for_eeg_stimuli.check_above_threshold(envelope_eeg)  # sends markers (should be fast)\n",
    "        track_for_eeg_stimuli.send_data_signal(envelope_eeg) # sends the signal (should also be fast!)\n",
    "        # query the threshold\n",
    "        # query the duration\n",
    "        \n",
    "        visual_markers_emg, audio_markers_emg = track_for_emg_stimuli.check_above_threshold(envelope_emg)  # sends markers (should be fast)\n",
    "        track_for_emg_stimuli.send_data_signal(envelope_emg) # sends the signal (should also be fast!)\n",
    "\n",
    "        # we should also pass on the track_for_eeg_stimuli and track_for_emg_stimuli to the GUI, as well\n",
    "        # as the function of the buttons that they press (lambda functions)\n",
    "        # so we know in our gui what is happening\n",
    "        \n",
    "        # make a matrix to put into the w_analysis...\n",
    "        analysis_data = np.vstack((\n",
    "            preprocessed_eeg,\n",
    "            envelope_eeg,\n",
    "            track_for_eeg_stimuli.thr * np.ones(preprocessed_eeg.shape)\n",
    "            visual_markers_eeg,\n",
    "            audio_markers_eeg,\n",
    "\n",
    "        )).T\n",
    "        \n",
    "        data_analysis.append(analysis_data)  # store it for later conversion\n",
    "        w_eeganalysis.update(analysis_data)  # put it onto the screen (but keep our on/off button...)\n",
    "        \n",
    "        \n",
    "        # collect the events that we generate - they are in w_analysis.MNEAnnotations\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    # after the loop is done -- we can close the windows (optionally) (or with code)\n",
    "    # and convert signals + analyzed signals into one MNE struct.\n",
    "    # best way would be to inspect a short recording and deal with allthe issues\n",
    "    # se also: w_analysis.GUI_Annotations\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
